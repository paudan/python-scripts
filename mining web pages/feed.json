[
 {
  "content": "Performing business analytics on the data lake using next-gen open source tools. \n This world is increasingly being driven by quantitative analysis, not qualitative. More than ever, corporate roles involved in decision-making need to have access to data and be able to make sense of it. — Thomas Nield , revenue management analyst and self-taught engineer at Southwest Airlines \n \n\n Many companies want to find more value by deploying analytics projects across the entire enterprise. Scaling analytics on the data lake, however, presents some tool and skill-related challenges. Primarily, the data lake itself is often not safe for mass use due to complexity, security, and governance issues. As such, access to it is usually restricted to engineering and IT teams who have the technical know-how and permissions to use it. \n\n This is not simply an access problem, however. On the business side, many analysts do not have the skill set needed to match data to organizational problems. They are also hampered by a lack of awareness of the data or tools available that could solve the problems that they’re seeing. \n\n As the big data ecosystem matures, business users will power their decisions with big data analytics, but it won’t happen automatically. To get there, users need the right tools, awareness, and skills to work in the data ecosystem, and companies need to adopt a  culture that fosters a creative data mindset. \n\n Jumping through the IT bottleneck \n\n For business users, the current process for getting insight from big data is often long and sometimes frustrating. \n\n According to Dr. Matt North , who comes from a risk analytics background at eBay, “Business analysts are often hogtied when it comes to big data and need to go through an IT approval process or sell their idea to their supervisor before they go anywhere.” \n\n If a business analyst wants to set up a data feed into Hadoop to investigate a particular business problem in detail, they have to get the right permissions and make the right requests for the data. Then, their project becomes IT’s project. Depending on the department and the project’s complexity, this process could take weeks, if not longer, to be completed. The asking, waiting, and following up creates friction and wastes time in deploying analytics projects. \n\n A big data tool for many users \n\n The current lack of access to data reflects the fact that most business users do not have the technical experience necessary to use the data lake, which is often built with open source tools like Hadoop and Spark that are not readily enterprise-friendly. Data lakes are also often disorganized, which makes finding data even more difficult. \n\n But big data does not have to remain the domain of power users—this is simply a limitation of the current tools. There is now a push in the marketplace to make the data lake more accessible to a wide variety of users. \n\n One tool that addresses a number of data lake access, governance, awareness, and skill issues is Kylo , a soon-to-be open sourced data lake orchestration framework based on Spark and NiFi. Kylo automates many functions related to data lakes, including data ingest, preparation, discovery, profiling, and management. The tool is accessed through a GUI built with the business analyst in mind, as well as modules for IT operations and data science. \n\n Through Kylo’s UI, business users can manipulate data they care about—creating feeds, defining ingests, wrangling data, transforming it, and publishing to target systems. They retain ownership of the project without needing to deploy any code or turn over control to IT, drastically reducing the amount of time these kinds of projects tend to take. \n\n For power users such as data scientists, data stewards and IT operations, Kylo provides metadata tracking, allowing them to make sure data is ingesting properly and understand its accuracy with Google-like search capabilities. These power users can create permissions and readily available templates for business analysts to use, in addition to monitoring and enforcing SLA policies. \n\n Though not a cure-all (especially for companies with a disorganized data lake), Kylo and products like it are interesting because they allow both business and power users to manipulate big data, bridging the gap between big data skills and access. Business users get insight faster, and IT can focus on engineering and data architecture problems instead of coding mundane routines. \n\n Making big data a culture fit \n\n Company culture should not be underestimated in its power to enable data analytics at scale. Successful data-driven companies do this by fostering an environment of exploration and awareness. \n\n Adopting such a culture relies on support from both the top and the bottom, whereby management encourages and enables experimentation, and individuals take initiative to learn about new tools and build products that solve problems of immediate interest to them. \n\n Building the right culture is a key step since, according to Dr. North, a lack of awareness of the tools and data available is one of the biggest challenges facing at-scale big data adoption in the enterprise today. Tools like Kylo are promising, but they do little good if business users don’t know they exist or how to use them. \n\n According to Thomas Nield at Southwest Airlines, “Data analysts must be resourceful and driven to not wait on others to provide what they need, and to develop whatever means they need to get something that may not even exist, data wise.” With tools like Kylo, for example, an analyst could discover relevant data sets in the catalog that he or she could then ingest and prep on their own before downloading a desktop analytics tool to work with them. \n\n The open source ecosystem is where this kind of self-driven learning thrives, since you don’t need to go through the process of acquiring—and paying for—a commercial tool in order to start using it or teaching others how to use it. \n\n While more feasible for some tools than others (for example, most users probably wouldn’t run a Hadoop cluster on their personal machine), the principle remains the same: learning new skills and tools from open source can be very enabling, and is becoming increasingly crucial in today’s business environment. \n\n Toward a data-ocracy \n\n The emergence of Kylo and other tools like it is paving the way for the next phase of big data—democratization. There is still a long way to go: in the vendor marketplace, tools need to be improved both to increase ease of use as well as to streamline analytics operations, with the goal of efficiently operationalizing models in the enterprise. \n\n In order to adapt, companies must embark on projects designed to enable business users with awareness of the data and tools that are available, adopt products that facilitate access, and encourage exploration and skill development. At the same time, business users should be proactive in searching out new tools and developing skills on their own. \n\n As tools like Kylo mature, everyone who wants to will be able to use data to make their jobs easier or more fun—hopefully both. \n\n This post is a collaboration between O’Reilly and Think Big, a Teradata company. See our statement of editorial independence . \n Continue reading Enabling enterprise-wide data analytics.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/QHUAQB9nq2s/enabling-enterprise-wide-data-analytics", 
  "title": "Enabling enterprise-wide data analytics"
 }, 
 {
  "content": "Information Cartography, Fighting Sexism, RPC History, and Reproducible Data Science \n \n Information Cartography: Creating Zoomable, Large-Scale Maps of Information (PDF) -- In this paper, we formalize characteristics of good zoomable maps and formulate their construction as an optimization problem. We provide efficient, scalable methods with theoretical guarantees for generating maps. Pilot user studies over real-world data sets demonstrate that our method helps users comprehend complex stories better than prior work. (via O'Reilly Data Show Podcast ) \n \n Val Aurora's Teaching Men to Fight Sexism (The Guardian) -- “It is the exact opposite of Lean In,” says Aurora. “Everything has been framed in terms of ‘what can women do to overturn sexism.' I have reframed it as ‘what can men do to stop sexism, because it is their responsibility.’” (via Zoe Corbyn ) \n \n Brief History of Distributed Programming RPC (Caitie McCaffrey, Christopher Meiklejohn) -- caution: contains CORBA. (via Caitie on Twitter ) \n \n Knowledge Repo -- AirBNB's open source next-generation curated knowledge sharing platform for data scientists and other technical professions . I've worked with data scientists ( Dragonfly Science ) who gave me their work in reproducible format, GitHub repo, and a Docker image that let me type make and create the PDF they mailed me as the results. It was fantastic, and converted me to data science reproducibility. (via Ricardo Bion ) \n \n Continue reading Four short links: 4 November 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/FjUbRrdjQM8/four-short-links-4-november-2016", 
  "title": "Four short links: 4 November 2016"
 }, 
 {
  "content": "Open source software is everywhere, but do you know where to start if you want to contribute, convince your manager your next project should be open source, or avoid recreating the wheel? Continue reading Open source 101.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-8TdFwsclaw/open-source-101", 
  "title": "Open source 101"
 }, 
 {
  "content": "A new approach to solving the reproducibility crisis. \n\n\n\n\n Open dissemination of scientific research and data is a prerequisite for solving the reproducibility crisis. The problem of low reproducibility is plaguing all disciplines, but its impact is much worse in preclinical research and development. Further clinical R&D into drug candidates and targets is turning up wasteful because published results can't be replicated (Freedman 2015, Begley 2012). There are similar problems with clinical research: Drug trials are published with mostly positive data supporting the claims, and negligible negative data (Ioannidis 2015). In this article, the discussion will mostly remain limited to preclinical research outcomes. \n\n The lack of replicability can be attributed to two broad reasons: a lack of consensus on protocols being used in research labs and the level of access to tools and equipment required for performing experiments. There are numerous methods that can be used to solve a problem, and domain-specific researchers often have their own versions of protocols which work optimally in their lab. This creates obvious problems for independent verification of results and claims. Moreover, not every lab has access to high-end equipment. Many researchers have to substitute lower-end techniques which may affect the overall accuracy. To solve these problems and standardize research equipment, we need the scientific community to reach a consensus on specific protocols and also provide generalized methods with sufficient flexibility for minor substitutions. Continue reading Blockchain-enabled open science framework.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5DKzBdOVD5c/blockchain-enabled-open-science-framework", 
  "title": "Blockchain-enabled open science framework"
 }, 
 {
  "content": "O'Reilly Radar Podcast: SNAFU Catchers, knowing how things work, and the proper response to system discrepancies. In this week's episode, O'Reilly's Mac Slocum sits down with Richard Cook and David Woods . Cook is a physician, researcher, and educator, who is currently a research scientist in the Department of Integrated Systems Engineering at Ohio State University, and emeritus professor of health care systems safety at Sweden’s KTH. Woods also is a professor at Ohio State University and is leading the Initiative on Complexity in Natural, Social, and Engineered Systems, and he's the co-director of Ohio State University’s Cognitive Systems Engineering Laboratory. They chat about SNAFU Catchers; anomaly response; and the importance of not only understanding how things fail, but how things normally work. Continue reading Richard Cook and David Woods on successful anomaly response.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/2egdSYQlmqU/richard-cook-and-david-woods-on-successful-anomaly-response", 
  "title": "Richard Cook and David Woods on successful anomaly response"
 }, 
 {
  "content": "The O’Reilly Data Show Podcast: Dafna Shahaf on information cartography and AI, and Sam Wang on probabilistic methods for forecasting political elections. In this special two-segment episode of the Data Show , I spoke with Dafna Shahaf , assistant professor at the School of Computer Science and Engineering at the Hebrew University of Jerusalem. Her area of research is focused on tools and techniques for overcoming information overload, an area of increasing importance in an attention economy . With the upcoming U.S. Presidential Elections right around the corner, I included a conversation between Jenn Webb , host of the O’Reilly Radar Podcast , and Sam Wang , co-founder of the Princeton Election Consortium and professor of neuroscience and molecular biology at Princeton University. Continue reading Visual tools for overcoming information overload.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BlIVnnt7RLs/visual-tools-for-overcoming-information-overload", 
  "title": "Visual tools for overcoming information overload"
 }, 
 {
  "content": "The O’Reilly Bots Podcast: Using data science to allocate campaign resources. The problem confronting a modern campaign manager is similar to the problem that any marketer encounters: how to spend finite resources to reach the right people and convince them to act. In this episode of the O’Reilly Bots Podcast , I talk data strategy with Andrew Therriault , chief data officer for the City of Boston. He was previously director of data science for the Democratic National Committee and is the editor of a free O’Reilly ebook called “ Data and Democracy: How Political Data Science is Shaping the 2016 Elections .” We talk about how data is changing today’s political campaigns—particularly the way that campaigns now determine which potential supporters to target with phone calls, mailings, and door-to-door contact. Continue reading Andrew Therriault on how data is transforming political campaigns.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/hzqXiHH_56A/andrew-therriault-on-how-data-is-transforming-political-campaigns", 
  "title": "Andrew Therriault on how data is transforming political campaigns"
 }, 
 {
  "content": "Safe Code, Defeating Face Detection, Data Fail, and Concept Graph \n \n Rules for Writing Safety-Critical Code -- like all general rules, you can find a situation to break each of these, but they're great defaults. \n \n Accessorize to a Crime (PDF) -- We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. \n \n \n 10 Ways Your Data Project is Going to Fail -- you don't have the right knowledge, you don't have the right people, you don't even have the right data. Let's give up and climb back up the trees, it's all been a terrible mistake. \n \n Microsoft Concept Graph -- concepts and categories, built off Probase from Microsoft Research. Knowledge in Probase is harnessed from billions of web pages and years' worth of search logs , an example of useful data exhaust from an unrelated app. \n \n Continue reading Four short links: 3 November 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/86I7HCc2iLA/four-short-links-3-november-2016", 
  "title": "Four short links: 3 November 2016"
 }, 
 {
  "content": "Five questions for Caskey Dickson: Insights on the changing role of IT ops. I recently sat down with Caskey Dickson, site reliability engineer and software engineer at Microsoft, to discuss the importance of strategic metrics, maximizing the customer experience, and how the role of operations is changing. Here are some highlights from our talk. \n 1. Why is it so important to think strategically about metrics? \n Metrics are the windows into the health and behavior of your service. Without the right metrics you won’t know what is going on or how to fix it. Furthermore, having metrics of the appropriate resolution and granularity is needed. If you only know a metric’s average fleet-wide then you have no idea if there are particular server instances that are failing your users. Can you tell the difference in error rates and latency broken down not just by server but by software revision? Having the right metrics presented in the right way provides a smoking gun that drives down TTD (time to detect) and TTM (time to mitigate) for the benefit of your customers. Peace of mind is just a bonus. Continue reading Systems engineering and the customer experience.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-zcbmqJ2RrA/systems-engineering-and-the-customer-experience", 
  "title": "Systems engineering and the customer experience"
 }, 
 {
  "content": "Learn about MIME types and how the browser and your JSON code interacts. Continue reading What is the correct JSON content type?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/uRbwZyWQ0Kg/what-is-the-correct-json-content-type", 
  "title": "What is the correct JSON content type?"
 }, 
 {
  "content": "Learn the best practices for adding JavaScript links to your code using buttons and divs. Continue reading What href attribute should you use for JavaScript-only links?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/NmaHgH-LYbw/what-href-attribute-should-you-use-for-javascript-only-links", 
  "title": "What href attribute should you use for JavaScript-only links?"
 }, 
 {
  "content": "A look at the data pipeline architecture for five key NERSC projects. Large-scale data management is essential for experimental science and has been for many years. Telescopes, particle accelerators and detectors, and gene sequencers, for example, generate hundreds of petabytes of data that must be processed to extract secrets and patterns in life and in the universe. \n\n The data technologies used in these various science communities often predate those in the rapidly growing industry big data world, and, in many cases, continue to develop independently, occupying a parallel big data ecosystem for science (see Figure 1). This post highlights some of these technologies, focusing on those used by several projects supported by the National Energy Research Scientific Computing Centre (NERSC) . Continue reading The big data ecosystem for science.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HRUG3_j0rhM/the-big-data-ecosystem-for-science", 
  "title": "The big data ecosystem for science"
 }, 
 {
  "content": "5 questions for Ryan Noon: Why your company needs a maker space, finding inspiration out the window, and what designers and elite athletes have in common. I recently asked Ryan Noon , designer at Nike, to discuss his journey into design, making, and the power of looking beyond the screen. At the O’Reilly Design Conference, Noon will be participating in a panel, Creating Space for Making in the Workplace , with Patrick Chew of IBM, Alexandra Williams of Airbnb, and Tim Belonax of Pinterest. \n\n What do you do at Nike? \n I run the Blue Ribbon Studio (BRS), which is our internal maker space, think tank, and school for Nike Design. I have a background in design and art—where I’ve worked for Alexander McQueen, Henrik Vibskov, and run my own collection, Ryan Noon . I came to Nike to work in design initially on apparel, then prints, than graphic design before I started the BRS. \n\n BRS is a design studio / maker space within the company that is open to the members of Nike Design. Within it are various studios that designers can think, prototype, create, explore, and play. We also run a BRS Academy within the space, which teaches everything from Laser Cutting 101 to Ikebana to Stonewashing to Sign Painting. Our goal is to invest in our amazing design talent—like any elite athlete, you need to practice and train to keep up your skill and talent; we feel the same about design. \n\n My background in fashion and working in various studios has all funneled into what I do today. My background in various fields from fashion, to print, to graphics, to running my own company, to growing up building and working with power tools have all helped bring life to the studio, as it encompasses  many disciplines. It’s a global / local studio, too—we love the creative community and want to make sure we have perspective from all around the world. \n\n You are a fashion designer by training. How did you find your way into the design world, and when did you know this was what you wanted to do? \n I was interested in all things art and design early on. I was into architecture first, then sailboat design, then fine art, and ended up studying fashion design and print at Central Saint Martins, in London. It was a great mix of design through fashion and art, through designing prints for the body. I used to go to visit my aunts in NYC during the summers who worked in design and hang out in the color design department and take drawing classes at Parsons. In New York, I also met with designer and artist Susan Cianciolo, who is a family friend; I was so inspired by her DIY and punk spirit of doing and making. \n\n You’re giving a talk with like minded designers from IBM, Airbnb, and Pinterest on creating space for making in the workplace. Why do you think it's important to offer this opportunity to employees? \n Being hands on is where the magic happens. Ideas don’t only come out on paper or through a laptop, but by making mistakes, experimenting, and talking to other people. Being in an energetic space with a diversity of disciplines adds to better product and ideas. \n\n What makes for a good designer in this day and age? \n Curiosity, hunger, self-initiative, knowledge of the future and the past. For me, not looking at a screen is key—when you’re in a taxi, look out the window; on the subway, see how people wear clothes—it’s way more inspiring than a blog. It’s important to think for yourself! Everything online has been (my least favorite word) “curated,” and it’s hard to form your own opinions from that much curation. There is inspiration in everyday things all around us—it’s a positive attitude and outlook that makes them exciting. Alchemy everywhere! \n\n You're speaking at the O'Reilly Design Conference in March. What sessions are you interested in attending? \n At a conference or festival I like to be surprised, I don’t usually read up too much and like to see as much as I can and wander into and experience all new things. \n\n\n Continue reading How to build great designers.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5IqZtIM6wrA/how-to-build-great-designers", 
  "title": "How to build great designers"
 }, 
 {
  "content": "Watch highlights covering security, defense, tools, and more. From the O'Reilly Security Conference in New York 2016. Experts from across the security world came together for the O'Reilly Security Conference in New York. Below you'll find links to highlights from the event. \n\n Once upon a future \n\n Heather Adkins explores various events in security history to show how we need to alter our course to change the future. \n\n \n\t Watch \" Once upon a future .\" \n \n\n Playing through the pain: The impact of secrets and dark knowledge on security and intelligence professionals \n\n Richard Thieme says the cost of security work and professional intelligence goes beyond dollars. It's measured in family life, relationships, and mental and physical well-being. \n\n \n\t Watch \" Playing through the pain: The impact of secrets and dark knowledge on security and intelligence professionals .\" \n \n\n A vision for future cybersecurity \n\n Rebecca Bace explains what we need to do to consolidate our efforts in cybersecurity so that we can instigate a new generation of techniques and applications. \n\n \n\t Watch \" A vision for future cybersecurity .\" \n \n Continue reading Highlights from the O'Reilly Security Conference in New York 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/uAn02SzlAYw/keynotes-from-security-conference-ny-2016", 
  "title": "Highlights from the O'Reilly Security Conference in New York 2016"
 }, 
 {
  "content": "Beyond MVP, Questioning Data, No Tech Industry, and Dealing with Resistance \n \n Getting Beyond MVP (Adrian Colyer) -- great advice for those whose prototype worked but now face scaling up on a rickety but small codebase. Adrian covers what to do next, to get you to safe and scalable without losing momentum. \n \n What's Wrong With Big Data? (James Bridle) -- excellent stuff, not usual Internet fare as the sentences and thoughts are complex. In short: big data isn't the panacea many think it is, has led to bad outcomes in many areas, and the simplistic approach it promotes may be actively dangerous. \n \n There Is No Technology Industry -- Rather than accepting that a company like Facebook, which knows more about our personal lives than any entity that’s ever existed, is simply “tech,” we should talk about it as an information broker, as an agent of government surveillance, as a media publisher, as a producer of unmanned drones, or in any other specific description that will assign appropriate accountability and context to their actions. \n \n \n Dealing with Resistance (Bill Martin) -- To deal with resistors you must understand their source of power. This power normally resides in one of four platforms. 1. Our organization has been down this path before. 2. Our organization may be committed, but the federal, state and regional offices are not. 3. Prove to me that the leadership team supports this. 4. If I go along with you, how long will you or I (us) be around? \n \n \n Continue reading Four short links: 2 November 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jG9gZCFfxC0/four-short-links-2-november-2016", 
  "title": "Four short links: 2 November 2016"
 }, 
 {
  "content": "Rebecca Bace explains what we need to do to consolidate our efforts in cybersecurity so that we can instigate a new generation of techniques and applications. Continue reading A vision for future cybersecurity.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/WlSHP9rILB8/a-vision-for-future-cybersecurity", 
  "title": "A vision for future cybersecurity"
 }, 
 {
  "content": "Heather Adkins explores various events in security history to show how we need to alter our course to change the future. Continue reading Once upon a future.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/z6Ndp1VVo2Y/once-upon-a-future", 
  "title": "Once upon a future"
 }, 
 {
  "content": "Richard Thieme says the cost of security work and professional intelligence goes beyond dollars. It's measured in family life, relationships, and mental and physical well-being. Continue reading Playing through the pain: The impact of secrets and dark knowledge on security and intelligence professionals.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Njhk_vaZRCU/playing-through-the-pain-the-impact-of-secrets-and-dark-knowledge-on-security-and-intelligence-professionals", 
  "title": "Playing through the pain: The impact of secrets and dark knowledge on security and intelligence professionals"
 }, 
 {
  "content": "Dan Kaminsky on the progress made at O’Reilly Security’s first hackathon to make web security easier. There’s a lot to be said about Internet security, and “could be easier” is definitely one of them. So, White Ops Labs and O’Reilly Security put out the call last week: Let’s get together, let’s write some code—all open source, with one clear directive—let’s make security easy. \n\n I’d like to specifically thank Code for America , who took our hacker invasion in stride. They were fantastic hosts, and like us, they’re trying to make a lot of unnecessarily difficult things much more achievable. Continue reading Let’s fix this thing: Results from our first hackathon.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/SzxpTrmycKw/lets-fix-this-thing-results-from-our-first-hackathon", 
  "title": "Let’s fix this thing: Results from our first hackathon"
 }, 
 {
  "content": "Close the time gap between analysis and action to bring about the next wave of improvements in efficiency and reliability—and magic. The number of devices that translate real-world events into a constant stream of zeros and ones continues to expand faster than ever. To account for this huge influx of new data, the technology used to capture, process, and analyze this information also needs to keep up. Embracing real time is the only option for companies seeking to elevate both the speed and the quality of information their business is analyzing. This article explores the importance of real time and the future of data analysis made possible by real-time technology in the form of both predictive analytics and machine learning, with a special focus on the world of alternative energy and the Industrial Internet of Things (IIoT). \n\n\n\n The world of renewable energy is a perfect example of the possibilities—and challenges—of the 21st-century economy. The promise of renewables is not just staunching the flow of environmental costs of the developed world’s lifestyle, but spreading economic growth globally without incurring these costs in the first place. The goal of transforming an industry to be more efficient and responsive is thrown into high relief in the case of energy. However, across all industries, the ability to intake, understand, and respond to data offers the possibility of radical change for the better, specifically in those very realms of efficiency and responsiveness. \n\n\n\n Up to now, the main benefits that data has brought to industries have been driven by analytical efforts separate from operational data pipelines. Business analysts examine data, find trends, propose tests, and deliver recommendations, which are then implemented by the operations side. But to continue gaining returns from data analysis, the timescales at which analysis operates have become finer and finer. In a transportation firm, for example, data analytics may be able to recommend new truck routes given traffic conditions. In order to implement those recommendations, however, the traffic conditions need to be recognized—and that analysis propagated to drivers—in real time. As “drivers” comes to mean “other computers,” this need remains central. \n\n\n\n The machine learning techniques that data scientists have developed over the past decades have immense power to categorize, predict, and detect anomalies. The first step to speeding up the time-to-delivered-insight is real-time data access and insights dashboards. The next step is to adapt machine learning techniques to real-time data—this means not only supplying incoming data for analysis, but also developing responsive algorithms that update on the basis of new data. \n\n\n\n Real-time analytics in the renewable energy industry \n\n As mentioned, the renewable energy industry offers a great example of using real-time analytics to deliver efficiency and responsiveness to everyday industry operations. As we replace fossil fuels with renewables (solar, wind, and hydro), and seek to use less energy overall, it is clear that the interplay of numerous machines, human analysts, and timescales necessitate real-time data pipelines. For much of the 20th century, the grid’s electrical needs could be satisfied by fossil-fuel-burning power plants. While daily and seasonal fluctuations could be predicted at a coarse level, plants would have to overproduce to ensure sufficient power to handle unexpected spikes in demand. Brownouts and surges were distressingly common, and of course, more fuel was burned than was needed. \n\n\n\n As renewables joined the grid, supply became much more complicated. While it takes a half hour or so to spin up a natural-gas turbine, solar panels and wind turbines depend on the weather; hydroelectric plants can be brought online in minutes, but need to pay attention to the long-term seasonal cycles of rain and snowfall, irrigation needs, and even fish migration. Meanwhile, the daily cycles of human energy needs are divorced from (and sometimes in conflict with) natural energy supplies: home air conditioners switching on in summer evenings as families return home, just as the sun sets and the wind dies down. \n\n\n\n Real-time analysis—and action on—data at the level of the individual energy source and the individual energy consumer is necessary to balance all these factors. Tracking supply and demand at this granular level enables the grid to balance supply and load by the second—a responsiveness that shows up in the consumer experience as reliability, with reduced or eliminated power outages and spikes. \n\n\n\n On the production side, data analysis provides reliability, too.  For example, one Internet of Things simulated application that tracks the status of nearly 200,000 wind turbines around the world shows how real-time data can empower businesses in the energy sector. This simulation provides not only real-time status dashboards, but also—crucially—predictive analytics that gives turbine operators insight into when machines are in danger of failing. This enables not only preventive maintenance, but predictive maintenance—servicing machines before they fail, and dispatching both personnel and material to where they’ll be needed. \n\n\n\n Where does real time go next? From ML to AI \n\n It’s almost hidden in that last paragraph, but bringing analytics into real time gets you a reach into the future, a bit of everyday magic. O’Reilly’s Mike Loukides and Ben Lorica recently discussed “ What is Artificial Intelligence? ” and pointed out that technologies creep from being “robots” or “AI” into our everyday lives as they’re implemented and become familiar. The bottom line to this transfer of technology from fantasy to reality is that it works seamlessly and in the same time frame as the real world. \n\n\n\n Real-time data analytics and machine learning provide the keys to deploying more and more services that offer “magical” abilities. By synthesizing not only vast amounts of information, but vast amounts of current information, real-time data contextualizes current observations with historical data, and delivers either action or augmentation to human decision-makers.  \n\n\n\n In today's data-driven world, it's easy to get caught up in the latest machine learning techniques or get deeper and deeper into feature engineering . In moments of sober realism, we spend time (commiserating over) data wrangling. But even this focus on the “dirty work” of data science misses the underlying data pipelines. The engineering work of getting data from sensors and users to databases and algorithms, and in turn getting analyses and decisions back to users (or actors like robots) is often the most important work in enabling data-driven action, especially in real-time contexts. \n\n\n\n This post is a collaboration between MemSQL and O’Reilly. See our statement of editorial independence . \n Continue reading Real-time data for the real world.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/4wQAVMXUpL8/real-time-data-for-the-real-world", 
  "title": "Real-time data for the real world"
 }, 
 {
  "content": "Is it possible for an AI to create revolutionary art? With the progress we've seen in artificial intelligence over the past year, there's been a lot of excited talk about AI and creativity. Can we make AI systems that are creative? And what does that mean? We've seen AIs that can write relatively formulaic newspaper stories, paint new Dutch Masters , apply Van Gogh's style to arbitrary pictures , and and play Go brilliantly . But is that creativity? As the clichéd question goes, \"Is it art?\" \n Questions about creativity are tough because we don't really know what \"creativity\" is. We have industries devoted to the production of formulaic pieces for mass consumption. We talk about the \"movie industry\" and the \"music industry,\" and those terms are absolutely right, because while movies and music are undeniably creative at some level, the industries are primarily about pushing out a lot of product and getting people to buy it. This year's \"the same\" isn't exactly the same as last year's \"the same,\" but art that's produced and marketed for mass consumption can't afford to be too different or strange. \n On the other hand, we have a notion that an artist is someone who creates taste rather than reflects it, someone who breaks the rules, who creates things of a kind that didn't exist before. These are the artists who create radical art. \n It's worth looking at a radically creative artist to see what this fuss is about. As a classical pianist, I'll choose Beethoven, who was probably the most radical composer in the history of music. What made Beethoven unique in the first place? Could a computer create music that's equally groundbreaking? \n Everybody is familiar with Beethoven's 5th symphony, and that's the problem: we're so familiar with it that we've forgotten how strangely it starts. The first movement is built entirely from a theme that's really just a four-note fragment. Da-da-da-DUH. Then those four notes again, in another key. And again, and again, and again. Where are you going to go with that? This is not Papa Haydn. \n Likewise, the Waldstein piano sonata has a 20 note theme , in which the first chord (C major) is repeated 16 times. Then the same chord again, but in another key. What's going on? Beethoven frequently takes a relatively uninteresting tune, breaks it down into tiny pieces, and reassembles it in amazing ways. What's he doing? He was never as good with a melody as Mozart; is he working from his weakness? Is he turning his deafness into an advantage? Maybe it's not surprising that a composer who was going deaf would start a piece by pounding out the same chord repeatedly. It's certainly no surprise that many of Beethoven's contemporaries considered this noise. \n The Opus 110 piano sonata is one of Beethoven's late masterpieces. Like most piano sonatas, it looks like it has three movements. But does it? The second movement really moves straight ahead into the third, without a break . The so-called third movement starts with a long bridge that nobody knows how to play (the notation in measures four and five doesn't make sense) into a lament. Then in measure 27, the lament suddenly becomes a fugue , also without a break. Then the lament comes back in the middle of measure 114; the fugue comes back in 137, with the theme turned upside down; and then all the parts scatter around in strange, uncoordinated ways to introduce a final section based on the fugue theme, but with big chords above a moving bass line. So: is this two, three, four, five, or six movements? \n I can't answer that question; there isn't an answer, and that's the point. Beethoven has turned the traditional sonata form into a problem. He's taken it as far as it can go, and moved way, way beyond it. He broke it, and has done something profoundly new. \n It wouldn't surprise me at all if an AI trained on Beethoven's works could \"write\" synthetic Beethoven symphonies or piano sonatas. I'm sure an AI would be good at breaking a larger theme up into parts and reassembling them in clever ways. It could certainly pick up characteristic gestures, like suddenly moving from fortissimo to pianissimo, or using the extreme ends of the piano's range. But the ability of AI to imitate isn't really the question. Imitation has never been all that hard. If you trained an AI with the entire body of Western music prior to Beethoven's birth in 1770, could it come up with something equally radical? Or, if you had all the music up until 1960, would you get Hendrix, the Beatles, and Zappa? All of these musicians were working out of traditions and styles that came before, whether the high classical music of Mozart and Haydn, or the blues of Leadbelly and Blind Lemon Jefferson. But they also took those earlier traditions, broke them apart, and built something new and different. \n But whether an AI can push an artistic tradition to a new level is only the start of the problem. Art is ultimately about humans: even if humans don't create it, they're the ones who look at it, listen to it, and decide whether or not they like it. With that in mind, it's worth thinking about the reception of any radical art. There's a common pattern: early on, a few cognoscenti love it, but the masses find it too difficult and hate it. Years or decades later, we worship it; then we find it boring. Chuck Berry's \"Roll over, Beethoven\" is ironically good commentary on this process: Beethoven became boring precisely because he was worshipped, and that worship transformed performances into stodgy, grandiose parodies of \"great art.\" But there's nothing unique about Beethoven. Talking about the decline of the music industry, Frank Zappa says : \n One thing that did happen during the 60s was some music of an unusual or experimental nature did get recorded or did get released. Now look at who the executives were in those companies at those times. Not hip young guys. These were cigar-chomping old guys who looked at the product that came and said, 'I don't know. Who knows what it is. Record it. Stick it out. If it sells, alright.' We were better off with those guys than we are now with the supposedly hip young executives who are making the decisions of what people should see and hear in the marketplace. \n We were lucky to get the Beatles; if the record execs hadn't said, \"I don't get it. What the hell.\", music would have taken a different course. Beethoven survived because, frankly, he was one of the biggest assholes in history, and nobody was going to tell him what to do. He had his fans amoung the avant garde, of course, but he certainly wasn't easy listening for your typical concertgoer. And he didn't care. \n Now we're at the heart of the problem of AI creativity. You can talk as much as you like about automating creativity, but the human is never out of the loop. At some point, humans have to look at whatever art works AI produces and decide whether they want more. That's not a technical requirement; it's a human one. If humans don't like it, they'll pull the plug. What happens to the algorithms when the listeners turn thumbs up or thumbs down? Will Spotify and Pandora (or, more precisely, their algorithms) take risks on challenging music? Or will the AI's managers tweak the algorithm and retrain until the results are more marketable? If an AI wrote the 5th symphony, would a music exec say \"pretty good, could be better, let's tweak the model and get something people will really like\"? \n It won't make a difference if the AI has a Beethoven-like ego. It won't have the ability to forge ahead regardless of what contemporaries think. It will be at the mercy of record execs and program managers who aren't likely to say \"Who knows what it is... if it sells, alright.\" Best case, we'll end up with occasional bits of brilliance sprinkled through yottabytes of unwanted music in cold storage. And nobody will have the time or patience to go through it. \n There's a deeper point here. Is it important that this art was created by humans? With music, you could argue that we only want something to entertain us. There is certainly room for music-as-product; you need something to listen to in elevators. And you could even argue that when you're listening to or performing Beethoven, you don't care where the music came from. But there are art forms for which that isn't true. Earlier this year, the Guardian published an article about computers writing poetry . Although these attempts were laughably bad (the Guardian called it Vogon Poetry ), we can assume that future attempts will be much better. But why is poetry important to us? Is it because it's a pleasing arrangement of words, or is it precisely because a human created those words? Music can be Muzak, but there's no poetic equivalent, except perhaps greeting card verse. Greeting cards can be written by AI: I really don't care. But I don't think I'd care about the poems of Keats or the plays of Shakespeare if they hadn't been written by a human. If Keats' Bright Star were just a pleasing arrangement of words coming out of a carefully tuned neural network, would we be interested in them? I don't think so. \n By the same token, I don't at all see why we need imitation Rembrandts or Van Goghs. Yes, there is a market for Elvis on Velvet, and perhaps even paintings in doctors' offices. But art as product gets boring quickly. While I'm sure that AI systems can produce an endless stream of pop songs, fake Dutch Masters, and sentimental poetry, and while AI may even be able to reproduce the relatively minor changes that go from period to the next, I don't think it's likely that AI will be able to produce revolutionary art. \n Could AI produce the next Beethoven, or the next Beatles, as opposed to just synthetic Beethoven or Beatles? I'm not saying that it can't, but that's a qualitatively different, and much more difficult, problem. At ZeitGeist, Mark Rumsen (as tweeted by Tim O'Reilly) said that \"Anyone who says they can tell you what the future of music is is either lying, or they are from the future.\" I certainly can't predict the future of music, or of any other art form. But neither can an artificial intelligence. I'm confident that great art will continue to break rules and leap out in front of human cognition. It will be about breaking with established norms and making something new. Making those leaps will be difficult (though maybe not impossible) for an AI. But appreciating and understanding those leaps will be even more difficult for the humans who ultimately have to decide whether to let the AI continue creating. Nobody pulled Beethoven's plug. \n     \n Continue reading Artificial creativity.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/fQPpvjem360/artificial-creativity", 
  "title": "Artificial creativity"
 }, 
 {
  "content": "Automatic Security Updates, Client Feedback, PWA Performance, and Beating Burnout \n \n Guide to Automatic Security Updates -- Automatic updates are a specific instance of a more general problem: secure code delivery. But in addition to the problems involved with securely delivering code from the developer to the user (with a healthy dose of distrust toward the delivery infrastructure), you have additional engineering challenges to contend with due to the automatic nature of the whole process. (via Scott Arciszewsk ) \n \n Client Feedback on Famous Designs -- brilliant. \n \n Lighthouse -- auditing and performance metrics for Progressive Web Apps. \n \n \n Practical Frameworks for Beating Burnout -- The overarching theme: “Do fewer things, but do them extremely well.” \n \n \n Continue reading Four short links: 1 November 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/nWho2lZcJDk/four-short-links-1-november-2016", 
  "title": "Four short links: 1 November 2016"
 }, 
 {
  "content": "Learn to serve web pages from the inexpensive and tiny ESP8266 module in a few lines of Arduino code. Continue reading How do you put a web server onto a tiny $10 Wi-Fi module?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/dirtQ1XiA_o/how-do-you-put-a-web-server-onto-a-tiny-wi-fi-module", 
  "title": "How do you put a web server onto a tiny $10 Wi-Fi module?"
 }, 
 {
  "content": "Five questions for Desiree Matel-Anderson: Insights on FIT’s three-step methodology for maneuvering through cybersecurity emergencies. I recently sat down with Desiree Matel-Anderson, chief wrangler of the Field Innovation Team (FIT) and CEO of the Global Disaster Innovation Group, to discuss designing in the security world with strengthened collaboration during a data breach. Here are some highlights from our talk. \n\n 1. As chief wrangler at FIT, a nonprofit focused on disaster preparedness and crisis, you see how many different groups respond to crises. What are some patterns you see in groups that successfully maneuver through emergency situations? \n\n The patterns of a successful emergency response are clear communication and collaboration fostered through simple design frameworks (i.e., using design as the method of problem solving). An example of this is FIT’s three-step prep design process, which is composed of this basic approach: \n\n \n\t Begin with a narrative of who and what you are solving for. \n\t Sculpt a focused challenge statement through our reframing exercise. \n\t Build out one concept to prototype. \n \n\n You begin the process with situational awareness of a cybersecurity breach to emphasize the importance of defining the environment, discussing with a team, and then laying out the details before jumping to conclusions. Next, you build on a challenge statement defining very specifically (a) who and what populations you are planning for, (b) which focused issue you will address, (c) and why this narrowed target is needed or important. Finally, you ideate and prototype in order to start brainstorming ideas for the security breach. This is an opportunity to come up with the wildest and most creative ideas possible, then ground your ideas back to reality by creating a physical representation using the resources around you. Teams are then formed to present the ideas to others to ensure that the logic behind the ideas is clear to those outside the group. \n\n 2. How would FIT’s three-step methodology to solve problems in crisis situations apply to a security breach? \n\n Our three-step prep design process can be utilized in any crisis situation. I’ll actually be leading our very first simulated hack at the O’Reilly Security Conference in Amsterdam. Participants will be interacting with fellow security professionals to practice design thinking and learn how these concepts can best be applied during a cybersecurity breach. The aim is better on-your-feet problem solving and rapid response during a security emergency. \n\n 3. What are the benefits of utilizing emergency management language in the case of a security breach? \n\n Collaboration is a pattern in emergency response that is paramount in all situations, including a security breach. It is essential that we all learn each other’s languages and understand the systems of our partners, which includes security professionals having a foundation of the language and structure of emergency management to support rapid response. \n\n 4. What principles define design-thinking methodology, and how do those principles apply to security? \n\n Design-thinking methodology is all about the end-user driving us towards empathy and placing ourselves in the shoes of the person impacted. By putting together creative solutions that drive impact for the end-user in the security field and/or any situation where challenges arise, you will have the three-step prep framework as a tool to build progressive solutions that you would not have believed possible and further drive an understanding of the cyber world around us and/or the other challenges we may face in our day-to-day activities. \n\n 5. As you mentioned, you're leading an interactive exercise on designing for cyber-security solutions at the O’Reilly Security Conference in Amsterdam this November. What presentations are you looking forward to attending while there? \n\n The presentations I am looking forward to attending are Google’s Allison Miller who has expertise in designing and implementing real-time risk prevention systems at Internet scale, and O’Reilly’s Courtney Nash who has a background in neuroscience and has focused on examining the way our brains interact with technology. Both of these women have fascinating backgrounds and a plethora of experience. I can’t wait to see them live! \n Continue reading Disaster preparedness for security professionals.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/04AfzL0HjSo/disaster-preparedness-for-security-professionals", 
  "title": "Disaster preparedness for security professionals"
 }, 
 {
  "content": "Faking Neural Nets, Watson Fintech, Changing Behaviors, and Time-Series Features \n \n Universal Adversarial Perturbations -- generating images that neural nets will mistakenly classify, and the system generalizes well across universal neural nets. \n \n RegTech -- IBM applying Watson to regulations. \n \n Knowledge-based Interventions are More Likely to Reduce Legal Disparities than are Implicit Bias Interventions -- argues that trying to educate about implicit bias doesn't change outcomes; instead, we should change people’s knowledge about how the structure of the social environment makes them complicit in the perpetuation of bias. Not sure how this will fare in today's individualistic world, where we aren't allowed to acknowledge that we operate in a cultural system of largely unspoken and completely arbitrary expectations, values, and other mutually constructed fictions. \"Don't teach THAT controversy!\" (via Patrick Forscher ) \n \n TSFRESH -- Automatic extraction of relevant features from time series. \n \n \n Continue reading Four short links: 31 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1D0JHyp2LsA/four-short-links-31-october-2016", 
  "title": "Four short links: 31 October 2016"
 }, 
 {
  "content": "The latest event revealed a pervading feeling that DIY biotech is maturing at an accelerating rate. \n\n\n\n\n This spring's IndieBio Demo Day, held as usual at the Folsom Street Foundry, felt distinctly different from its predecessors. The microbrews still flowed freely if not excessively, and the finger foods were elegant and savory. And as with past Demo Days, the place was packed. People were standing cheek by jowl, save for the fact that very few jowls were in evidence among the largely under-35 crowd. \n But the ambience had somehow shifted. Earlier Demo Days were charged with a giddy anticipation of a great but unknown future. The air seemed to crackle with static electricity and ozone. Comparisons with Silicon Valley in the early 1980s were widespread, and they felt apt. Continue reading Inside IndieBio Demo Day.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/zR67H_s2p-g/inside-indiebio-demo-day-2016", 
  "title": "Inside IndieBio Demo Day"
 }, 
 {
  "content": "Join Dan Kaminsky at the O’Reilly Security Hackathon to help make web security easier and more effective. A lot of security is just making computers behave like we think they do: effective security reduces the complexity of systems. Well-defined interfaces, known good state. This has been my mantra for a while—know what’s communicated, know what’s stored. We think we do this already; we have all sorts of very nice flow diagrams, a request goes in, a response goes out, and anything stored goes into the database. It’s simple, easy, and generally incorrect. But security can, and absolutely will, become more effective. \n\n How we got here \n\n We have systems leaking memory over time, and network services willing to communicate a bit too much. Quietly, there’s been a significant increase in the difficulty of converting memory corruption—where a system’s behavior becomes “undefined”—into controlled, well redefined attacker controlled code. What used to take days really does take months now. We’ve made things much harder for attackers but not much harder for defenders. It’s not a zero sum game. \n\n Computers are different now than they were even just a few years back. We didn’t used to have a quarter terabyte of RAM, or thousands of parallel processors, or clouds willing to manage microservices by the instantiation. These sorts of shifts have completely changed how software is written and deployed (and maybe eliminated the difference between the two). We need to adapt our security strategies to keep pace with all this change. \n\n What we’re fixing \n\n The driving demand of the first O’Reilly Security Hackathon really is: Make Security Easy. Possible is not enough. I think at this point every developer has tried some API, where there are a hundred possible ways to do something, and maybe one that actually works. Meanwhile, a delightful variety of misleading error messages serves to obscure the one working path. And that is how APIs die. Security is not remotely special in this regard. \n\n We’ve been exploring how to move fast and fix real security issues in the hackathon, and it’s been a lot of fun. We didn’t used to have a way to automatically certify a computer’s identity. With Let’s Encrypt (aka CertBot), we do now. So we’ve been hacking on Jump to Full Encryption (JFE) and writing code that just certifiably encrypts everything—Apache, NGINX, MySQL, MongoDB, Java, Node.js, Python; it just works. \n\n You don’t configure IP addresses and DHCP leases and DNS servers for each random component in your infrastructure. Security’s been a lot harder than insecurity: a lot more expensive, a lot more difficult, a lot more error prone. But it’s the same job, for all of these services. You know what’s great at doing the same job over and over again? Computers. \n\n Calling all coders \n\n There’s so much to work on. Maybe you can help us. Solutions don’t have to happen in full, instantly; compromises are still effective. I for one would like to see credit card breaches that only affect 500 customers. Along with JFE, there are properties of the cloud that we’re exploring under the Ratelock project . Constraining and precisely defining our potential losses is a theme of that work; we’re hoping to create an actual, effective, and performant isolation technique. \n\n We’re also trying to do something about the drudgery of handling the attacks that are starting to consume our networks. Remediation has to be accelerated or there won’t be a network to remediate. With Overflowd, for every one out of a million packets a tracer goes to the source and destination. It piggybacks on existing netflow monitoring infrastructure, provides abuse contact data, anti spoof, a bunch of neat properties. We need to aim traffic at it and see what happens. \n\n If it all sounds crazy, good. The software security status quo is not OK and nothing good should sound sane in the context of it. We’re here to write code and see what it can do. Move fast and fix things. Let’s make security easy. \n\n I hope to see you at the Hackathon in San Francisco (happening now), and at the O’Reilly Security Conferences in New York and Amsterdam , or of course, on this Internet that we’d really like to keep from burning . \n Continue reading Move fast and fix things.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/F65fddFtwAo/move-fast-and-fix-things", 
  "title": "Move fast and fix things"
 }, 
 {
  "content": "As the dust settles on last week's DDoS attack, it’s time to ask—and act on—the question of “Now what?” When we announced our new O’Reilly Security Conference this past spring in an article titled “ Building better defenses ,” we started by discussing how we’re barraged daily with inflammatory news stories about the seemingly desperate state of security in the U.S. and abroad. Never has this been more true than with last week’s coverage of the massive DDoS attack on Dyn . \n\n To be clear, we’re not suggesting that this event wasn’t newsworthy. The breadth of the attack, the sheer numbers of devices involved, the business, monetary, and long-term effects of this attack were all worth the frontpage headlines by mainstream media that they received. This attack affected many significant enterprises , millions of users, and quite frankly, it felt ominous. It has been suggested that the perpetrators of this attack were experimenting with taking down the Internet . It also garnered the attention and outrage (if only briefly) of the general public. \n\n We can hope the awareness created by this event denotes another step in the march toward security being taken more seriously—not by security professionals who already understand the importance, but by others who have such a profound effect on ultimate security posture. The greater awareness of threats leads to opportunities for focused and effective security conversations. In respect to this particular event, there’s an obvious business case to be made for funding a backup DNS service. \n\n If you’re struggling with how to have those conversations with either the larger business unit or with members of your organization in general, we’ll have some great presentations at our upcoming events in the human element track (“ Security FORCE: A model for highly reliable security behaviors and cultures ,” “ Security by consent ”) and the bridging the gap between security and business track (“ Continuous security ,” “ Link complex regulation to practical security ”), that address these challenges directly. Full talks and sessions will be also available after the events on Safari .  \n\n Everything is (still) on fire  \n\n Ultimately though, while the details differ, this narrative is hardly new. A malicious actor (still unknown) used nefarious means ( the Mirai botnet ) to wreak havoc, this time on our beloved Internet. Outside of those very focused steps forward mentioned above, the truth is that the security landscape is largely the same as it was a week ago. In-the-trenches security professionals weren’t surprised by this turn of events. They’ve been decrying IoT security for as long as the IoT has existed. Nor will security pros be surprised by the next, likely larger attack . Defenders aren’t the shocked masses. They’re Nostradamus in the server room foretelling crises to come. \n\n As the headlines fade and public attention turns back to other issues, we return to a problematic status quo. The dust is settling and the same challenges that existed before this latest news cycle—insufficient resources, communication gaps between security and other stakeholders, and technical challenges, to name just a few—remain, which leads us to the eternal question: Now what?  \n\n We’ve joined the fire brigade (and we brought a truck) \n\n That constant echo of “Now what?” along with its counterpart “How can we help?” has defined our recent and upcoming initiatives in the security realm. We felt not only a need, but a responsibility, to offer our resources to the effort. Because asking thoughtful questions has tremendous power, but change only comes when those questions are followed by action. \n\n To that end, we’re hosting our first hackathon this week in San Francisco with Dan Kaminsky, Chief Scientist at White Ops . With this event, we aim to make TLS/HTTPS trivial to deploy. We see the value and the importance of making strong isolation of insecure code a smooth experience. Dan wants to put better DevOps in place so that people can find and fix bigger things faster, and we saw the opportunity to help. Let’s break the status quo. Let’s make security easy(er). \n\n We’ll be unveiling the results of this hackathon next week at the O’Reilly Security Conference in New York. We’ll also be talking about the broader security landscape, with an aim to address the most common problem areas in defensive security—the same immutable problems that keep coming up in our discussions with the defensive security community at large. We saw the opportunity to have a broad positive impact on security: to create content, events, and a structure for helping individuals, organizations, and the security community build and use the tools they need to make our world safer. Of course, we’re not doing this alone. We’re immensely appreciative of our League of Extraordinary Defenders for their role in shaping and supporting the event. \n\n If you’d like to learn more about our security initiatives: \n\n \n\t Subscribe to the O’Reilly Security newsletter . \n\t Follow us on Twitter . \n\t Join us in person in New York and Amsterdam . \n \n Continue reading Breaking the security status quo: Let’s make security easy(er).", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xgZqQIC1ebo/breaking-the-security-status-quo", 
  "title": "Breaking the security status quo: Let’s make security easy(er)"
 }, 
 {
  "content": "Engineering Management, Object Spreadsheets, Microservices Hindsight, and Slack Shaping \n \n Management in 10 Tweets (Marc Hedlund) -- My most frequently given pieces of management advice. \n \n \n Object Spreadsheets -- an enhanced spreadsheet tool with support for storing and manipulating structured data. End-user developers can use it directly to work with a data set, or to build a web application that offers constrained view and update access to a larger population of users. \n \n \n What I Wish I'd Known Before I Scaled Uber to 1,000 Services (YouTube) -- watch before you get seduced by the microservices press machine. Everything from failure modes to logging floods and (twitch) migrations. \n \n Shaping Your Slack -- Rand's advice for conventions and configuration that make good communities. \n \n Continue reading Four short links: 28 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/UGBHvONCmQQ/four-short-links-28-october-2016", 
  "title": "Four short links: 28 October 2016"
 }, 
 {
  "content": "Five questions for Chiara Rustici: Insights on the global impact of the new General Data Protection Regulation, and how to get started with implementation. I recently sat down with Chiara Rustici, General Data Protection Regulation (GDPR) expert and independent consultant and analyst, to discuss the EU’s new GDPR requirements and their impact on organizations worldwide. Here are some highlights from our talk. \n\n 1. Could you provide some context on the GDPR for readers who aren’t familiar with it? \n\n This is a new legal framework for handling personal data of EU-based individuals, be they customers, prospects, contractors or employees. It is already in force but not yet enforceable—businesses and not-for profit organizations have until May 25, 2018 to prepare. Although GDPR originates in the EU, it actually impacts businesses worldwide: If they handle personal data of EU individuals, or do business with organizations that do, the GDPR imposes obligations on how that data is treated, even if that personal data has traveled outside the EU and is now stored and handled in a distant corner of the world. \n\n 2. What are the broad sweeping implications of the GDPR for organizations? \n\n One new legal requirement is to report a personal data breach within 72 hours of becoming aware of it, both to data protection supervisory authorities and to the individuals themselves if the breach significantly affects their rights. This requirement is made more daunting by the fact that the GDPR expands the definition of personal data to include any information that has even the potential , alone or in conjunction with other information, to identify a person. \n\n To comply with GDPR, there are three key areas where businesses and software architects need to do things differently. They need to find a way to: \n\n \n\t \n Preserve the identity of an individual across the multitude of their different descriptions, names and properties, and keep these together under one heading. Building a personal data ontology is a huge undertaking, and the GDPR adds complexity to the task by requiring that all the disparate data points that can potentially identify a single individual be included. \n\t \n Preserve the link between a data point and the individual that data point can potentially identify. This is either because you need to preserve a consent chain, or because you need to advise the individual when the purpose for handling a particular data point changes. \n\t \n Attach a sunset clause / purpose achieved clause to personal data points. Personal data should only be employed for a specific purpose. Once that purpose is superseded, the data point in question should no longer be held by the business. Business applications are not equipped with automatic erasure options at the person-centric level of granularity. New thinking is needed to figure out, for example, how to deal with a file that contains personal data of multiple individuals when your purpose for handling the personal data has “expired” for one person but not for the others. \n \n\n 3. How would you recommend an organization begin preparing for the GDPR? What are the first steps? \n\n The GDPR is, in business terms, a cost/benefit analysis exercise: It asks organizations to make choices about what they want out of the personal data they collect and what they are prepared to do or stop doing in order to pursue their business goals in a lawful manner. In no way can IT professionals start the GDPR compliance journey on their own, but they will need to get involved in business strategy conversations, asking “why?” quite a lot: \n\n \n\t Why exactly are we archiving this data instead of just erasing it? \n\t Why are we building this data lake? \n\t Why are we allowing the design of this app to collect these categories of personal information? \n \n\n They need to escalate to the board and CEO the key data business model questions: \n\n \n\t What are we trying to achieve with these personal data sets? \n\t Are the expected financial gains higher than the costs of encrypting or de-identifying the sets? \n \n\n Ultimately, the key preparation stage is a thorough business review of the privacy landscape; some data-driven business models will no longer be viable in a GDPR world. This is a non-negotiable boardroom-level privacy posture call. Security professionals know too well that not storing personal data is the ultimate security strategy for this very hazardous asset; now the GDPR offers them powerful legal grounds to impose a limit on collection and storage. \n\n 4. What barriers are organizations facing in implementing GDPR protocols? \n\n Quite a few, but I’ll focus on the top three cultural barriers here because it takes longer to shift organizational cultures than it takes to develop new technologies. These are: \n\n \n\t \n A user design culture. Years of design thinking have produced wonderfully intuitive user interfaces (UI), but tap-and-swipe, drag-and-drop actions, second nature for us all, amount to moving data sets in and out of corporate perimeters and often out of national boundaries. Data flows are invisible, and the GDPR wants to make them visible again. Unlearning our UI instincts, and pausing to think “Is this cut-and-paste job going to make living people identifiable?” will be tough. \n\t \n A boardroom culture. Years of thought leadership on digital transformation have produced a corporate culture of over-collecting personal data without a corresponding discipline of measuring the return on investment. Personal data brings with it a cost and a risk the moment it enters an organization: It is a liability long before it becomes an asset. The GDPR has placed a precise and hefty price tag on the cost and risk of handling personal data so that, in addition to ending indiscriminate collection and lax security practices, hopefully, it will encourage more disciplined treatment of the enterprise value of personal data. \n\t \n Vendor/contractor culture. Nineteen months to enforcement date, I am not aware of any large IT household name that has a GDPR-compliant or GDPR-proof product or service on the market. My guess is that, right until the end, GDPR-defying products will continue to be bought until the GDPR emergency is so obvious to all that it becomes a compelling reason to upgrade or buy new hardware and software with GDPR options. What makes matters worse is that the businesses with enough foresight to start the GDPR compliance journey early end up paying for the research and development costs of the new GDPR-compliant offerings by these same vendors. \n \n\n 5. You're speaking at the O’Reilly Security Conference in Amsterdam this November . What presentations are you looking forward to attending while there? \n\n All of them! I am a bona fide conference junkie and find it exhilarating to attend large events where brains come together and faces light up all the time in those very precious “ah-ha” moments. I make it a point of attending the ones I do not even grasp the title of—if I already understand 80% of the words and concepts in the presentation, I'm not learning and am sitting in the wrong room! \n Continue reading How the EU’s GDPR affects all of us.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/CcjB5AmRgi8/how-the-eus-gdpr-affects-all-of-us", 
  "title": "How the EU’s GDPR affects all of us"
 }, 
 {
  "content": "Five questions for Rafał Kuć: Insights on what sets the ELK stack apart from other log management solutions, common user pitfalls, and tips for getting started. I recently sat down with Rafał Kuć, search consultant and software engineer at Sematext Group, to discuss the benefits and common pitfalls of using the ELK stack to manage logs. Here are some highlights from our talk. \n\n 1. What is the ELK stack? \n\n ELK stands for Elasticsearch, Logstash and Kibana. The trio, which was once separate, joined together to give users the ability to run log analysis on top of open sourced software that everyone can run for free. \n\n \n\t Elasticsearch is the search and analysis system. It is the place where your data is finally stored, from where it is fetched, and is responsible for providing all the search and analysis results. \n\t Logstash, which is in the front, is responsible for giving structure to your data (like parsing unstructured logs) and sending it to Elasticsearch. \n\t Kibana allows you to build pretty graphs and dashboards to help understand the data so you don’t have to work with the raw data Elasticsearch returns. \n \n\n It doesn't matter if you are running a modest company producing small mobile games or a large enterprise—ELK can come in handy when you need time-based data analysis. \n\n 2. Why is it so useful for dealing with logs? And what makes it different from other solutions for managing logs? \n\n Visibility is the key here. Having hundreds of servers running different applications, virtual machines, containers—all that adds up to a lot of data that needs to be analyzed when problems happen or when you need to understand how things work. Being able to narrow down your data or easily find the information you are looking for really helps with operations-related tasks. Adding metrics to the equation gives you even more visibility compared to logs only. When you see your metrics correlated to logs you have the full picture—not only of what is happening now, but also about the history and how your software pieces were behaving. \n\n I think there are two main things that make ELK so popular and different. First, it’s simple to use and is very DevOps friendly—especially the Elasticsearch part; it’s manageable with a great rest API and easy setup. Second is the pricing. If you are a small company, want to build things in house, and can’t afford enterprise-level log analysis solutions, ELK is great—if, of course, you can pay the price of managing it yourself. \n\n 3. What do people often get wrong with the ELK stack? \n\n We see a variety of problems when we help clients at Sematext. Every case is different, and it is very dependant on the knowledge and experience of the user. Those who just started their adventure with logs usually run into data structure problems, so-called mappings in Elasticsearch. Mappings define what fields are there in your documents and what the behavior of the fields is. Depending on the field configuration, it can or can't be used with certain functionalities. (For example, a numeric field can be used for range aggregation, which can divide our search system query logs by their latency—like queries that were running up to 10 milliseconds, from 11 to 100 and so on. You can't do that on text data.) More advanced users often run into scaling- and performance-related problems, like how to use the hardware in the most efficient way, how many servers to have, whether their setup is really the best way to approach their problem, and so on. \n\n 4. What's the best way to get started using the ELK stack with your infrastructure? \n\n Just start using it, really. There are lots of good tutorials on how to begin. And you don't even need to start with tens of servers to see how useful it will be for you. If your logs are not standard, get an example, like Apache (very known, very simple to work), and just run Logstash, put your data to Elasticsearch and start building your visualizations using Kibana. It may not be easy to build complicated dashboards initially, but the more practice you have, the easier it gets. Just start. \n\n 5. You're speaking at the Velocity Conference in Amsterdam this November. What presentations are you looking forward to attending while there? \n\n There are multiple talks I would like to attend, especially related to container orchestration, Internet of Things, monitoring, and understanding data. Examples of talks I can't wait to attend are \" How humans see data \" by John Rauser from Snapchat, and \" Kubernetes and Prometheus: The beginning of a beautiful friendship \" by Björn Rabenstein from SoundCloud. \n Continue reading Understanding the ELK stack.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/lBtaHfpo3Jg/understanding-the-elk-stack", 
  "title": "Understanding the ELK stack"
 }, 
 {
  "content": "Open source software is everywhere, but do you know where to start if you want to contribute, convince your manager your next project should be open source, or avoid recreating the wheel? Continue reading 12 patterns for hypermedia service architecture.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/xBgl7_swq2Q/12-patterns-for-hypermedia-service-architecture", 
  "title": "12 patterns for hypermedia service architecture"
 }, 
 {
  "content": "Exploring the many trends in business, education, and government that have contributed to the current state of open source activity in Brazil. \n Open Source in Brazil: Growing Despite Barriers\n \n\n\n Foi pesado o sono pra quem não sonhou \n\n   Continue reading Open source in Brazil.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/HTaQZ6E1D9k/open-source-in-brazil", 
  "title": "Open source in Brazil"
 }, 
 {
  "content": "A​ ​hands-on guide​ ​that​ ​demonstrates several storage APIs in action.​ Let’s Build Something! \n\n Now   that you’ve seen multiple types of client-side storage techniques as well as some libraries to help make using them easier, let’s build a real, if simple, application that makes use of some of these techniques. Our application will be a tool for a company intranet (“Camden Incorporated”—coming to the NYSE soon) that lets you search for your coworkers. This could be built using a traditional application server model, but we’ve decided to build something fancy using modern web standards. To make the search near instantaneous, we’ll use client-side storage to keep a copy of the employee database on the user’s browser. This, of course, opens up all kinds of interesting issues. \n\n First off, how do we handle   synchronization? Companies aren’t static. People join or leave companies all the time. How often that happens, of course, depends on the company itself, but obviously you have to consider some form of strategy for keeping the user’s copy of data in sync with the real list on the server. Luckily, in our scenario we don’t have to worry about user edits. The server side is always “truth,” which means we can ignore changes on the client side when syncs happen. For our demo we’re not going to worry about syncing at all, but in a real-world demo your application server could provide an API where the client says—and by “says” I mean via code, of course—“My copy of the data was last updated on October 10, 2015 at 8:55 AM.” The server could then respond with a set of changes that have occurred since that date. Those changes could cover deletions (people who left the company), changes (people getting married and changing their name, or getting new titles), and additions (new hires). The client-side code would apply those changes and then make a note of the current time so that the next time it speaks to the server it can correctly receive the changes. Continue reading How to build an app with client-side storage.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/F3d_TmRJLIM/how-to-build-an-app-with-client-side-storage", 
  "title": "How to build an app with client-side storage"
 }, 
 {
  "content": "Overcoming the dearth of labeled data, deployment issues, and regulation fears to increase the use of AI in health care. Why is the world’s most advanced AI used for cat videos , but not to help us live longer and healthier lives? Here, I’ll provide a brief history of AI in medicine, and the factors that may help it succeed where it has failed before. \n\n Imagine yourself as a young graduate student in Stanford’s Artificial Intelligence Lab, building a system to diagnose a common infectious disease. After years of sweat and toil, the day comes for the test: a head-to-head comparison with five of the top human experts in infectious disease. Your system squeezes a narrow victory over the first expert, winning by just 4%. It handily beats the second, third, and fourth doctors, and, against the fifth, it wins by an astounding 52%. Continue reading 3 challenges for artificial intelligence in medicine.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/1o8-UuWlsD0/3-challenges-for-artificial-intelligence-in-medicine", 
  "title": "3 challenges for artificial intelligence in medicine"
 }, 
 {
  "content": "The O’Reilly Design Podcast: Design education, mentoring, and what design skills matter the most. In this week’s Design Podcast, I sit down with Danielle Malik , designer, owner, and mentor at Design Equation. We talk about mentoring the next generation of designers, what she is learning from recent design grads, and the role fear can play in our work. Continue reading Danielle Malik on mentoring the next generation of designers.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9vlNcKb6Kz4/danielle-malik-on-mentoring-the-next-generation-of-designers", 
  "title": "Danielle Malik on mentoring the next generation of designers"
 }, 
 {
  "content": "The O’Reilly Bots Podcast: Bots are the new web. In this episode of the O’Reilly Bots Podcast , Pete Skomoroch and I recap O’Reilly Bot Day , held October 19, 2016, in San Francisco. The event gave us a good picture of what the bot community—and bot landscape—looks like, and the diverse group of attendees conveyed a strong sense of optimism about bots. Slides from Bot Day presentations are available here . Continue reading Shivon Zilis on the machine intelligence landscape, and Bot Day wrap-up.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/jtyZRTPIf0o/shivon-zilis-on-the-machine-intelligence-landscape-and-bot-day-wrap-up", 
  "title": "Shivon Zilis on the machine intelligence landscape, and Bot Day wrap-up"
 }, 
 {
  "content": "Device Time, Ingenious Hardware, Parsing JSON, and Tech Awesome \n \n Family Device Use Plan -- American Association of Pediatrics made a guide based on revised data in papers in the Pediatrics journal, which gives you plenty of options about when and where you can return your developing minds to the real world. This guide is best accessed via your mobile phone as you walk along a busy city street with headphones on. (via On Limiting Children's Screen Time ) \n \n Traveling Salesman Problem Solved With Magnets (Stanford) -- this is the press release that explains the Science paper. Ingenious computation through lasers, simulating magnetics, representing the constraints to which a solution must be found. \n \n Parsing JSON is a Minefield -- a variation of \"the great thing about standards is there's so many of them to choose from,\" JSON is defined in at least six different documents and it just gets sadder from there. \n \n Self-Folding Origami Robots -- because there's not a single word in that title that isn't freaking awesome by itself and together they're the Bacon-Wrapped Chocolate-Dipped Deep-Fried Coffee of tech. \n \n Continue reading Four short links: 27 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/8hTdJUAy-jw/four-short-links-27-october-2016", 
  "title": "Four short links: 27 October 2016"
 }, 
 {
  "content": "Tips for writing a successful proposal for the O'Reilly Security Conference. Recently, we provided an inside look at the call for proposals (CFP) process for the O’Reilly Security Conference . In that post, we focused on how we managed CFP review, the metrics from our 2016 events, and our overall approach to shaping the conference. Now, we’re sharing common traits of the highest-reviewed proposals to help you successfully navigate the CFP process next year. \n\n We’ve both been on the program committee for several conferences, and were thrilled to have such a strong set of proposals to work with this year. If you didn’t make it in for O’Reilly Security NY or O’Reilly Security Amsterdam , here are some common themes we saw differentiating the top-tier talks: \n\n 1. Propose a practical, defense-focused talk \n\n We’ve built our new O’Reilly Security Conferences to unite in-the-trenches defensive security practitioners, to open discourse on defending organizations, and to provide a forum for sharing concrete solutions. Want to talk about breaking? Great! Then, flip the topic around and tell us how to fix it. We all know that there are problems. Let’s talk about solutions. \n\n 2. Offer a novel or creative idea, approach, or opinion \n\n With so many talks to choose from, the ones that really stood out included novel (if practical) approaches. There were many topics (hello, machine learning) that were popular in the CFP. If we received multiple talks on the exact same subject, we had to make some difficult choices about which presentations should be chosen. Consistently, the program committee prioritized presentations with a fresh approach. Take heed. \n\n 3. Focus on what the audience will gain \n\n There are a lot of security conferences out there that deliver amazing research. We’re trying something a little different—a focus on pragmatic approaches for practitioners. So we tried to focus on building an agenda that delivers on the promise to provide attendees with practical, useful tips they can apply immediately. That means that talks that highlighted what the attendees would learn grabbed our attention, over talks that simply described a point of view, concept, or research finding. Also, proposals that made it easy for us to understand what the speaker was going to share with the audience picked up bonus points over proposals that only dropped hints. (Save the surprises for Ally’s birthday extravaganza.) \n\n 4. Don’t pitch \n\n While the conference marketing team works closely with a number of conference sponsors, the program committee sticks to a purely editorial process. The O'Reilly-ans describe this as \"separation of church and state\" and we think strict adherence to this principle yields the best results for everyone involved. On the program committee side, we err very much on the side of technical talks with lessons learned and practical takeaways. If you are proposing a talk about a product (notably your own), teach attendees useful things and demonstrate their credibility. \n\n Attendees are then likely to look at your product. Pitch them and they'll instead be reluctant to look at the product, and learn less in the process. \n\n 5. Write well \n\n We’re not copyediting your proposal. Ain’t nobody got time for that. But we do care whether you’ve thought carefully about what you’d like to present, and whether you’ve clearly articulated that focus. Also, you will ultimately be presenting this talk in front of a crowd, with slides, which we do suggest you also carefully edit. When you’re tasked with reviewing 400+ proposals, the sloppy ones are usually the first to go. \n\n 6. Be brief and avoid jargon \n\n While common jargon within proposals did spur some colorful outbursts (and at least one drinking game among program committee members), we’re less interested in jargon and much more interested in solving long-standing problems. And we can all agree, there are plenty of those to keep us busy. It was the focused, buzzword-free proposals that consistently received strong ratings. \n\n The CFP for the 2017 O’Reilly Security Conference will open sometime next spring. Sign up for O’Reilly’s weekly security newsletter or follow @oreillysecurity on Twitter to be alerted when the CFP window opens. \n Continue reading 6 ways to hack the O’Reilly Security Conference CFP.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/6WWw3wMaSkc/6-ways-to-hack-the-oreilly-security-conference-cfp", 
  "title": "6 ways to hack the O’Reilly Security Conference CFP"
 }, 
 {
  "content": "Five questions for Björn Rabenstein: Insights on Kubernetes, Prometheus, and more. I recently sat down with Björn Rabenstein, production engineer at SoundCloud and co-developer of Prometheus , to discuss Google-inspired tools and why Kubernetes and Prometheus work so well together. Here are some highlights from our talk. \n\n 1. Why are Kubernetes and Prometheus such a good match? \n\n Intriguing, isn't it? The match is so good that people sometimes claim Prometheus was specifically created to monitor Kubernetes clusters. However, they were developed completely independently. Only in early 2015 did Prometheans and Kubernauts meet for the first time to connect the dots. \n\n In part, it's a case of convergent evolution. My current employer SoundCloud, where most of the initial Prometheus development happened, needed a container orchestration solution for their growing microservice architecture, and they needed to monitor all of that. No on-premise solutions were readily available, so they had to be created. Our in-house solution for container orchestration was obviously pre-Kubernetes and even pre-Docker. In view of the recent developments, we have deprecated it and are migrating to a Kubernetes setup. However, monitoring our old container orchestration system and the services running on top of it is structurally very similar to monitoring Kubernetes and the services on it. Very few steps had to be performed to come up with a native integration of Kubernetes and Prometheus. \n\n Another part of the answer to your question is what I call the shared spiritual ancestry of both, or, as a colleague phrased it: “They are twins separated at birth.” Kubernetes directly builds on Google's decade-long experience with their own cluster scheduling system, Borg. Prometheus's bonds to Google are way looser but it draws a lot of inspiration from Borgmon, the internal monitoring system Google came up with at about the same time as Borg. In a very sloppy comparison, you could say that Kubernetes is Borg for mere mortals, while Prometheus is Borgmon for mere mortals. Both are “second systems” trying to iterate on the good parts while avoiding the mistakes and dead ends of their ancestors. \n\n And by the way, Kubernetes and Prometheus are both Greek 10-letter words. But that is pure coincidence. \n\n My talk at Velocity Amsterdam will cover the technical aspects of pairing Prometheus with Kubernetes in more detail. Stay tuned. \n\n 2. Kubernetes and Prometheus are both tools that are inspired by tools used internally at Google. What other tools are out there? \n\n When I joined Google more than 10 years ago, it was like entering a science fiction movie, so fundamentally different was most of the technology used. The only glimpse I had taken in advance was reading the papers on MapReduce and GFS . While Google engineers certainly still live far in the future, I daresay that the technological differences to “normal” mid-size tech companies are much less fundamental these days. The existence of the many Google-inspired tools and technologies out there are in a way both a cause and a consequence of that development. \n\n A growing number of projects were just directly open-sourced by Google, with Kubernetes being a prime example. You can learn a lot about how Google works internally from technologies like protocol buffers and gRPC , and even from the Bazel build system or from the Go programming language . \n\n Another source of inspiration are the many whitepapers Googlers have published (like the aforementioned GFS and MapReduce papers) or just the brains of ex-Googlers that miss the awesome infrastructure after leaving the company. (Rumors that Google would use Men-in-Black-style neuralyzers during offboarding are strongly exaggerated.) In that way, we got the whole Hadoop ecosystem, various other implementations of key-value stores with BigTable semantics, Zipkin and OpenTracing , or CockroachDB . In the metrics space, Prometheus deserves less credit than you might think. I see the data model used by Prometheus as the first principle from which everything else follows. That data model was brought to the world by OpenTSDB as early as 2010 (by a former teammate of mine). Prometheus “merely” added an expression language to act on the data model for graphing and alerting, and a collection path so that metrics reliably find their way from their source in the monitored targets into your monitoring system. \n\n I'm sure I have forgotten many other tools that have deserved to be mentioned here. \n\n 3. Using Kubernetes effectively requires a good understanding of new modes of application development: cloud-native practices, distributed systems, etc. What sort of mindset change is needed to make the best use of Prometheus, particularly when compared to more traditional monitoring tools? \n\n The most important paradigm shift is from hosts to services. Not only are single-host failures increasingly likely in large distributed systems, but those systems are explicitly designed to tolerate single-host failures. Waking somebody up in the middle of the night because a single host has stopped pinging is not sustainable. Your monitoring system has to be able to view the service as a whole and alert on an actual impact of the user experience. In the words of Rob Ewaschuk , symptom-based alerting is preferred over cause-based alerting. At the same time, you still need to be able to drill down to explore causes. Jamie Wilkinson nailed that pretty much in one sentence, quoting from Google's SRE book : “We need monitoring systems that allow us to alert for high-level service objectives, but retain the granularity to inspect individual components as needed.” \n\n Traditional monitoring is pretty much host-based, with Nagios being the proverbial example. While you can try to bend your traditional monitoring system toward more modern ideas, you will bang your head into a wall eventually. \n\n A nice example for a post-Nagios approach toward service-based monitoring is StatsD . It was a huge step forward. However, it has certain issues concerning details of its design and implementation, but most importantly it misses out on the second half of Jamie's sentence above: How do I go back to inspecting individual components? \n\n I’ll take that as a segue to the Prometheus data model, which I briefly touched on before. In short: Everything is labeled (as in Kubernetes— hint! ). Instead of a hierarchical organization of metrics, as you might know it from a typical Graphite setup, labeled metrics in combination with the Prometheus expression language allow you to slice and dice along the labeled dimensions at will. Metrics collection happens at a very basic level, aggregation and other logical processing happens on the Prometheus server, and can be done ad hoc , should you be in a situation where you want to view your metrics from different angles than before. Most commonly, that happens during an outage when there is really no time to reconfigure your monitoring and wait for the newly structured data to come in. \n\n Finally, you really need white-box monitoring, ideally by instrumenting your code. While the idea of black-box probing has many merits, and you should definitely have a moderate amount of black-box probing in your monitoring mix, it is not sufficient for a whole lot of reasons. Fortunately, instrumentation for Prometheus is fairly easy. Very little logic and state is required on the side of the monitored binary. As mentioned above, the logic (like calculating query rates or latency percentiles) happens later on the Prometheus server. \n\n 4. People sometimes refer to these tools as Google Infrastructure for Everyone Else (GIFEE). Google has some very unique challenges; how can one use these particular tools for their own organizations, which may not have the same issues, particularly with regard to scale? \n\n This is a fascinating topic. SoundCloud has not much more than 100 engineers. Google has more than 10,000. I would guess that similar ratios apply to the traffic served by each company. We are talking about roughly two orders of magnitude difference in scale. That's huge. Still, SoundCloud–and many other similar companies–are big enough to venture into an area where many lessons can be learned from giants like Google. You will certainly need to translate those lessons quite a bit, but the basic ideas are nevertheless applicable. Google's book about Site Reliability Engineering is a great source of wisdom. The whole GIFEE thing made it so much simpler to put ideas from the book into concrete action. Still, you should resist the temptation to blindly copy “how Google would do it.” You need to carefully consider what's different in your organization, in terms of scale, infrastructure, workflows, culture, etc. and then decide how the underlying principle translates into concrete actions within your own framework. GIFEE gives you the tools; it's up to you to use them in ways appropriate for your organization. In case you like anecdotal evidence: It worked out quite nicely for SoundCloud. \n\n 5. You're speaking at the Velocity Conference in Amsterdam this November. What presentations are you looking forward to attending while there? \n\n Glad you asked because that made me check out the whole schedule and plan where to go in advance. On the other hand (as a true Promethean) I really hate one-dimensional metrics and picking just a few presentations from so many that are interesting for various reasons. \n\n With event logging being another important pillar of monitoring (besides metrics, what Prometheus is for), I really look forward to the two talks about using Elasticsearch for monitoring. Another topic close to my monitoring heart is a good understanding of what anomaly detection can and cannot accomplish, and I hope that Peter Buteneers's hateful lovestory will cover that. “ Unsucking your on-call experience ” and “ Breaking apart a monolithic system safely without destroying your team ” are ventures SoundCloud has gone through, too, and I'm curious about the experiences of other organizations. Finally, I always enjoy Astrid Atkinson's presentations very much. I have fond memories of the very impressive onboarding session she gave to my group of Nooglers (i.e., new Google employees) back in 2006. She is a great role model. \n Continue reading Google infrastructure for everyone else.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/rosLUdGJNA8/google-infrastructure-for-everyone-else", 
  "title": "Google infrastructure for everyone else"
 }, 
 {
  "content": "Watch highlights covering software architecture, microservices, distributed systems, and more. From the O'Reilly Software Architecture Conference in London 2016. Experts from across the software architecture world came together in London for the O’Reilly Software Architecture Conference . Below you'll find links to highlights from the event. \n\n The evolution of software architecture \n\n Mark Richards discusses the factors that have enabled the evolution of software architecture over the past three decades and explores what the future of software architecture might look like. \n\n \n\t Watch \" The evolution of software architecture .\" \n \n\n Microservices: Pros and cons \n\n Taking opposing sides, Rachel Laycock and Cassie Shum debate the merits of microservices. \n\n \n\t Watch \" Microservices: Pros and cons .\" \n \n\n Listening to the design pressures \n\n Martin Thompson explores emergent designs and working practices that succeed in areas where the design pressures quickly cull that which does not deliver. \n\n \n\t Watch \" Listening to the design pressures .\" \n \n\n The architecture of uncertainty \n\n Kevlin Henney explains how uncertainty, lack of knowledge, and options can be used to drive a system’s architecture and its development schedule. \n\n \n\t Watch \" The architecture of uncertainty .\" \n \n\n High-performance teams \n\n Patrick Kua explores why and how software architects should care about high-performance teams. \n\n \n\t Watch \" High-performance teams .\" \n \n\n Am I only streaming? Thinking Reactive \n\n Rob Harrop looks at how we can use the concepts of CSP, Actors, and Reactive as powerful tools for reasoning about our systems. \n\n \n\t Watch \" Am I only streaming? Thinking Reactive .\" \n \n Continue reading Highlights from the O'Reilly Software Architecture Conference in London 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/pnTfMcOyhXM/keynotes-from-software-architecture-london-2016", 
  "title": "Highlights from the O'Reilly Software Architecture Conference in London 2016"
 }, 
 {
  "content": "The O’Reilly Security Podcast: Building cathedrals, empowering the watchers, and breaking out of the security monoculture. In this episode, I talk with Brendan O’Connor, a security researcher, lawyer (but not your lawyer) and owner of security consulting firm Malice Afterthought . We discuss creating a culture that celebrates collaborative teamwork over harried heroes, how monitoring and checklists really can save lives, and breaking out of the security monoculture. Continue reading Brendan O’Connor on security as a monoculture.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/cr3df-jzJls/brendan-oconnor-on-security-as-a-monoculture", 
  "title": "Brendan O’Connor on security as a monoculture"
 }, 
 {
  "content": "Jim Kalbach explains how he uses maps as collaborative tools, highlighting the importance of value alignment, and demonstrates how to engage others in workshops and exercises. Continue reading Visualizing Value: How to Align Teams with Experience Mapping.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/CongIf-_FbU/visualizing-value-how-to-align-teams-with-experience-mapping", 
  "title": "Visualizing Value: How to Align Teams with Experience Mapping"
 }, 
 {
  "content": "Python and R are widely accepted as logical languages for data science—but what about Go? If you follow the data science community, you have very likely seen something like “language wars” unfold between Python and R users. They seem to be the only choices. But there might be a somewhat surprising third option: Go , the open source programming language created at Google. \n\n\n\n In this post, we are going to explore how the unique features of Go, along with the mindset of Go programmers, could help data scientists overcome common struggles. We are also going to peek into the world of Go-based data science to see what tools are available and how an ever-growing group of data science gophers are already solving real-world data science problems with Go. \n\n\n\n Go, a cure for common data science pains \n\n Data scientists are already working in Python and R. These languages are undoubtedly producing value, and it’s not necessary to rehearse their virtues here, but, looking at the community of data scientists as a whole, certain struggles seem to surface quite frequently. The following pains commonly emerge as obstacles for data science teams working to provide value to a business: \n\n\n \n \n Difficulties building “production-ready” applications or services: Unfortunately, the very process of interactively exploring data and developing code in notebooks along with the dynamically typed, single-threaded languages commonly used in data science cause data scientists to produce code that is almost impossible to productionize. There could be a huge amount of effort in transitioning a model off of a data scientist’s laptop into an application that could actually be deployed, handle errors, be tested, and log properly. This barrier of effort often causes data scientists’ models to stay on their laptops or, possibly worse, be deployed to production without proper monitoring, testing, etc. Jeff Magnussen at Stitchfix and Robert Chang at Twitter have each discussed these sorts of cases. \n\n \n Applications or services that don’t behave as expected: Dynamic typing and convenient parsing functionality can be wonderful, but these features of languages like Python or R can turn their back on you in a hurry. Without a great deal of forethought into testing and edge cases, you can end up in a situation where your data science application is behaving in a way you did not expect and cannot explain (e.g., because the behavior is caused by errors that were unexpected and unhandled). This is dangerous for data science applications whose main purpose is to provide actionable insights within an organization. As soon as a data science application breaks down without explanation, people won’t trust it and, thus, will cease making data-driven decisions based on insights from the application. The Cookiecutter Data Science project is one notable effort at a “logical, reasonably standardized, but flexible project structure for doing and sharing data science work” in Python—but the static typing and nudges toward clarity of Go make these workflows more likely. \n\n \n An inability to integrate data science development into an engineering organization: Often, data engineers, devops engineers, and others view data science development as a mysterious process that produces inefficient, unscalable, and hard-to-support applications. Thus, data science can produce what Josh Wills at Slack calls an “ infinite-loop-of-sadness ” within an engineering organization. \n \n\n\n Now, if we look at Go as a potential language for data science, we can see that, for many use cases, it alleviates these struggles: \n\n \n\n Go has a proven track record in production, with widespread adoption by devops engineers, as evidenced by game-changing tools like Docker , Kubernetes , and Consul being developed in Go. Go is just plain simple to deploy (via static binaries ), and it allows developers to produce readable, efficient applications that fit within a modern microservices architecture. In contrast, heavy-weight Python data science applications may need readability-killing packages like Twisted to fit into modern event-driven systems and likely rely on an ecosystem of tooling that takes significant effort to deploy. Go itself also provides amazing tooling for testing, formatting, vetting, and linting (gofmt, go vet, etc.) that can easily be integrated in your workflow (see here for a starter guide with Vim). Combined, these features can help data scientists and engineers spend most of their time building interesting applications and services, without a huge barrier to deployment. \n\n Next, regarding expected behavior (especially with unexpected input) and errors, Go certainly takes a different approach, compared to Python and R. Go code uses error values to indicate an abnormal state, and the language's design and conventions encourage you to explicitly check for errors where they occur. Some might take this as a negative (as it can introduce some verbosity and a different way of thinking). But for those using Go for data science work, handling errors in an idiomatic Go manner produces rock-solid applications with predictable behavior. Because Go is statically typed and because the Go community encourages and teaches handling errors gracefully , data scientists exploiting these features can have confidence in the applications and services they deploy. They can be sure that integrity is maintained over time, and they can be sure that, when something does behave in an unexpected way, there will be errors, logs, or other information helping them understand the issue. In the world of Python or R, errors may hide themselves behind convenience. For example, Python pandas will return a maximum value or a merged dataframe to you, even when the underlying data experiences a profound change (e.g., 99% of values are suddenly null, or the type of a column used for indexing is unexpectedly inferred as float). The point is not that there is no way to deal with issues (as readers will surely know). The point is that there seem to be a million of these ways to shoot yourself in the foot when the language does not force you deal with errors or edge cases. \n\n Finally, engineers and devops already love Go. This is evidenced by the growing number of small and even large companies developing the bulk of their technology stack in Go. Go allows them to build easily deployable and maintainable services (see points 1 and 2 above) that can also be highly concurrent and scalable (important in modern microservices environments). By working in Go, data scientists can be unified with their engineering organization and produce data-driven applications that fit right in with the rest of their company’s architecture. \n \n\n\n Note a few things here. The point is not that Go is perfect for every scenario imaginable, so data scientists should use Go, or that Go is fast and scalable (which it is), so data scientists should use Go. The point is that Go can help data scientists produce deliverables that are actually useful in an organization and that they will be able to support. Moreover, data scientists really should love Go, as it alleviates their main struggles while still providing them the tooling to be productive, as we will see below (with the added benefits of efficiency, scalability, and low memory usage). \n\n The Go data science ecosystem \n\n Ok, you might buy into the fact that Go is adored by engineers for its clarity, ease of deployment, low memory use, and scalability, but can people actually do data science with Go? Are there things like pandas, numpy, etc., in Go? What if I want to train a model—can I do that with Go? \n\n\n\n Yes, yes, and yes! In fact, there are already a great number of open source tools, packages, and resources for doing data science in Go, and communities and organization such as the high energy physics community and the coral project are actively using Go for data science. I will highlight some of this tooling shortly (and a more complete list can be found here ). However, before I do that, let’s take a minute to think about what sort of tooling we actually need to be productive as data scientists. \n\n\n\n Contrary to popular belief and as evidenced by polls and experience (see here and here , for example), data scientists spend most of their time (around 90%) gathering data, organizing data, parsing values, and doing a lot of basic arithmetic and statistics. Sure, they get to train a machine learning model on occasion, but there are a huge number of business problems that can be solved via some data gathering/organization/cleaning and aggregation/statistics. Thus, in order to be productive in Go, data scientists must be able to gather data, organize data, parse values, and do arithmetic and statistics. \n\n\n\n Also, keep in mind that, as gophers, we want to produce clear code over being clever (a feature which also helps us as scientists or data scientists/engineers) and introduce a little copying rather than a little dependency. In some cases, writing a for loop may be preferable over importing a package just for one function. You might want to write your own function for a Chi-squared measure of distance metric (or just copy that function into your code) rather than pulling in a whole package for one of those things. This philosophy can greatly improve readability and give your colleagues a clear picture of what you are doing. \n\n\n\n Nevertheless, there are occasions where importing a well-understood and well-maintained package saves considerable effort without unnecessarily reducing clarity. The following provides something of a “state of the ecosystem” for common data science/analytics activities. See here for a more complete list of active/maintained Go data science tools, packages, libraries, etc. \n\n Data gathering, organization, and parsing \n\n Thankfully, Go has already proven itself useful at data gathering and organization, as evidenced by the number and variety of databases and datastores written in Go, including InfluxDB , Cayley , LedisDB , Tile38 , Minio , Rend , and CockroachDB . Go also has libraries or APIs for all of the commonly used datastores (Mongo, Postgres, etc.). \n\n\n\n However, regarding parsing and cleaning data, you might be surprised to find out that Go also has a lot to offer here as well. To highlight just a few: \n\n\n \n \n GJSON —quick parsing of JSON values \n\n \n ffjson —fast JSON serialization \n\n \n gota —data frames \n\n \n c svutil —registering a CSV file as a table and running SQL statements on the CSV file \n\n \n scrape —web scraping \n\n \n go-freeling —NLP \n \n\n Arithmetic and statistics \n\n This is an area where Go has greatly improved over the last couple of years. The gonum organization provides numerical functionality that can power a great number of common data science related computations. There is even a proposal to add multidimensional slices to the language itself. In general, the Go community is producing a some great projects related to arithmetic, data analysis, and statistics. Here are just a few: \n\n\n \n \n math —stdlib math functionality \n\n \n gonum/matrix —matrices and matrix operations \n\n \n gonum/floats —various helper functions for dealing with slices of floats \n\n \n gonum/stats —statistics including covariance, PCA, ROC, etc. \n\n \n gonum/graph or gograph —graph data structure and algorithms \n\n \n gonum/optimize —function optimizations, minimization \n \n\n Exploratory analysis and visualization \n\n Go is a compiled language, so you can’t do exploratory data analysis, right? Wrong. In fact, you don’t have to abandon certain things you hold dear like Jupyter when working with Go. Check out these projects: \n\n\n \n \n gophernotes —Go kernel for Jupyter notebooks \n\n \n dashing-go —dashboarding \n\n \n gonum/plot —plotting \n \n\n\n In addition to this, it is worth noting that Go fits in so well with web development that powering visualizations or web apps (e.g., utilizing D3 ) via custom APIs, etc., can be extremely successful. \n\n Machine learning \n\n Even though the above tooling makes data scientists productive about 90% of the time, data scientists still need to be able to do some machine learning (and let’s face it, machine learning is awesome!). So when/if you need to scratch that itch, Go does not disappoint: \n\n\n \n \n sajari/regression —multivariable regression \n\n \n goml , golearn , and hector —general purpose machine learning \n\n \n bayesian —bayesian classification, TF-IDF \n\n \n sajari/word2vec —word2vec \n\n \n go-neural , gonn , and neurgo —neural networks \n \n\n\n And, of course, you can integrate with any number of machine learning frameworks and APIs (such as H2O or IBM Watson ) to enable a whole host of machine learning functionality. There is also a Go API for Tensorflow in the works. \n\n Get started with Go for data science \n\n The Go community is extremely welcoming and helpful, so if you are curious about developing a data science application or service in Go or if you just want to experiment with data science using Go, make sure you get plugged into community events and discussions. The easiest place to start is on gophers slack , the golang-nuts mailing list (focused generally on Go), or the gopherds mailing list (focused more specifically on data science). The #data-science channel is extremely active and welcoming, so be sure to introduce yourself, ask questions, and get involved. Many larger cities have Go meetups as well. \n\n\n\n Thanks to Sebastien Binet for providing feedback on this post. \n Continue reading Data science gophers.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/B7g7aUrEVwo/data-science-gophers", 
  "title": "Data science gophers"
 }, 
 {
  "content": "Running your image classifier in your own iOS application. In TensorFlow for Poets , I showed how you could train a neural network to recognize objects using your own custom images. The next step is getting that model into users’ hands, so in this tutorial, I'll show you what you need to do to run it in your own iOS application. \n\n You’ll find written instructions in this post, along with a screencast with commentary showing exactly what I am doing. Continue reading TensorFlow for mobile poets.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/AMzrQ5lRb0E/tensorflow-for-mobile-poets", 
  "title": "TensorFlow for mobile poets"
 }, 
 {
  "content": "Don’t underestimate this emergent property of your organization. Culture has been a topic of interest ever since some sage declared that it eats strategy for breakfast (or lunch— the exact origins of the maxim are murky ). While there are numerous guides to the three, five, or seven steps to an ideal culture, I suggest that senior executives should first concentrate on my golden rule of culture: don’t mess it up. \n\n As a leader, your every move is watched and your every word is measured for deeper meaning. That means it is easy to screw up an organization’s culture, even when you have the best of intentions. I recently spoke with the head of a large organization who remarked that he hadn’t realized how “loud” his voice became when he stepped into the top job. He told me that when he mentioned in passing that X sounded interesting, suddenly people were forming X teams and sending him reports on the future of X. He didn’t intend for all of that to happen, but when someone is in his position, people put a lot of weight on every word. He had to learn to be careful with everything he said and did. \n\n This is an example of how executives often overestimate their control of culture yet underestimate their influence in it. What does that mean? Culture is an emergent property of the system that is your organization. It is composed of every person, process, and protocol in the organization. Every decision adds to or detracts from the desired outcome. Your culture is the sum of a thousand little actions. No executive can dictate culture through a memo, speech, or decree. Nor can you “set it and forget it” because it is constantly evolving. At each step, be informed and intentional. \n\n Yet, an executive can use what he or she controls to greatly influence the tone and tenor of the organization. Doug Conant , former CEO of Campbell Soup Company, noted the importance of each person’s contribution by marking accomplishments with handwritten notes. As many as 30,000 of them over his tenure according to one estimate . Taking the time to recognize significant contributions fosters a culture of mutual respect. I found a similar practice at a hospital outside of Boston that had committed to differentiating itself in a highly competitive marketplace through a culture of service. The handwritten notes were sent by supervisors to workers at their home addresses. Experiencing the feeling of receiving a personal gesture did more to build the culture of service than any point system or other gimmick ever could. \n\n So, walk the talk: if you want a culture where time is valued, show up to meetings on time and require an agenda. Punctuality will spread. If you want a culture that values a positive vibe, be ruthless about moving out toxic individuals no matter how great their other talents. If you don’t, people will assume that it is OK to be a jerk (see Bob Sutton ’s excellent work on “ The No A**hole Rule ”). Doug Conant will be delighted if you follow his lead in sending handwritten notes. \n\n There are executives who delight in a cutthroat culture. There are others who think that culture is a bunch of mush—“We have an annual golf outing, don’t we?” In my experience, far more frequent are executives with benign intentions who make missteps with unintentional consequences for the organization’s culture. \n\n Physical space matters to team communication \n\n A company I know well recently moved to a new neighborhood. The area is a little edgy but on the rise. The new space is light-filled and airy. The workspaces are fluid. Yet, multiple people have told me that just a few months after the move, the company feels more silo-ed. Functional stovepipes have hardened and cross-unit collaboration waned. However, thinking across organizational boundaries remains a mantra at the company. What happened? \n\n The answer seems to lie in the physical space. The executives made smart choices to move beyond the elegant cube farm feel of their former digs. However, their old home offered a unique architectural feature: a five-floor, glass-encased spiral staircase. Open seating and coffee stations surrounded the staircase on each floor. This core transit corridor was a natural place to bump-and-connect with people from elsewhere in the company. In contrast, the next-generation space has great bones but lacks that space to build critical connective tissue through spontaneous interactions over coffee or lunch. At one Fortune 500 company I visited, they intentionally decided not to have an executive dining room. Instead, top executives mingled with more junior employees in a common cafeteria. It set the tone of approachability that permeated the culture. \n\n The consequences of physical space choices for organizational culture are as undeniable as they are, sometimes, unintended. In my experience, executives too often accept the limitations of layout and structure because they view it as a one-time decision and look only at short-term construction costs. \n\n Across the country, I saw a counter-example. The CEO of a technology company gave me a tour of his facility, and on one floor, we encountered a pile of construction debris. He apologized for the mess and explained that they were always altering their physical space to better match the tasks to be performed. The costs were minor, he said, when compared to the value of the work produced. He also noted that the landlord thought he was a bit crazy, as he was the only tenant regularly making such adjustments. My host thought the other tenants were simply short-sighted: “I don’t want any of my engineering teams underperforming because of something I can fix this easily,” he said. \n\n Even without hammers and saws, there are steps you can take. An executive from another tech company faced a similar challenge with communication across groups. However, he solved it by programming the company’s meeting room reservation system so that people could only book conference rooms outside of their home zone. So, if you sat on One East, you might find your weekly staff meeting being held on Two West. This forced people to walk through and to different parts of the building. It’s a bit like thwarting high school cliques—sometimes you have to give people a little shove to help them meet someone new. \n\n Learning from failure will improve agility \n\n If failure to appreciate the power of physical space is one culture-damaging pitfall, legacy processes are another often overlooked by senior executives. \n\n Many years ago, management thought leader Adrian Slywotzky explained to me that an invisible wall exists in most businesses: in front of the wall are the customer-facing operations. Here, change is constant and often rapid in response to market demands. Behind the wall are administrative functions such as finance, human resources, and the rest. Change tends to come much more slowly. Companies invest in and expect adaptation to meet customer needs in front of the wall yet see back-end functions as evergreen. Little return is perceived from innovation behind the wall and, in fact, functions such as accounting may be governed more by laws and regulations than shifts in the marketplace. So, there are natural and legitimate reasons for each to evolve at different speeds. However, this can also lead to significant dysfunction. You will only be so agile in front of the wall if a Soviet-style bureaucracy lurks behind it. \n\n Incentive structures are one example of this phenomenon. Rewards, tangible and intangible, are powerful cultural signals. It does no good to admonish people to act like entrepreneurs yet distribute raises and bonuses evenly across the organization (yes, this still happens). When there is little or no reward for individual effort, you’re creating a non-entrepreneurial environment. The exception to this is when you have established a significant company-wide goal and the reward is clearly tied to achieving it. In this case, you may want to acknowledge that your accomplishment required the efforts of everyone from the security guards to the CEO. Crafting a new incentive system is a significant undertaking, though the cultural costs for foregoing the effort can be brutal. \n\n Similarly, if every investment decision requires a visit to Vice President Nada and then a presentation to the Risk Aversion Committee, you will not build a culture of creativity and innovation. If the projections of success have to be too rosy in order to survive the process, you will either find great ideas disappearing or an emergent proficiency in gaming the system to garner approval. A better approach is to make it easy to try new ideas on a small scale and teach people how to appreciate and learn from failure. Those are properties of a culture where innovation thrives. \n\n A vast array of these processes course through your organization like blood vessels. The lesson is to be aware of these evolutionary cycles to ensure that the back end remains relevant to and supportive of what is happening on the front end. Otherwise that dynamic culture you think you are fostering in front of the wall is likely to be dragged down by stagnation and misalignment behind it. \n\n Be accountable \n\n How do you successfully adhere to my golden rule? First, don’t delegate culture to HR or any other department. Own it . Be accountable for it. Second, articulate it —preferably through an inclusionary process with input from throughout the organization. One of my favorite examples is Herman Miller’s Things that Matter to Us . Finally, live it . Look at all your major decisions, processes, and protocols through the lens of culture just as you submit them to financial and regulatory scrutiny. Make culture as important to the red light/green light verdict as more traditional criteria. Hire and promote for fit as well as competence. Listen carefully for what resonates and what causes dissonance in the organization. Be open to feedback—even when it hurts a bit. \n\n We may never learn who first said that culture beats strategy. All that really matters is in a world where agility and speed are prized, the statement is true more often than not. Strategy is the plan, but culture is the energy that animates it. That’s reason enough to work hard to not f it up. \n Continue reading How leaders can build a great culture: Own it, articulate it, live it.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0KjL8J4LXuo/how-leaders-can-build-a-great-culture-own-it-articulate-it-live-it", 
  "title": "How leaders can build a great culture: Own it, articulate it, live it"
 }, 
 {
  "content": "The O’Reilly Hardware Podcast: How connected devices are leading to energy and cost savings. In this episode of the Hardware Podcast , I speak with Mark Wright, director of product management at Samsung Strategy and Innovation Center , and Darren Beck , author of the newly-published O'Reilly ebook \" Smart Business: Gaining an Edge Through IoT-Powered Sustainability .\" Continue reading Mark Wright and Darren Beck on the IoT and sustainability.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/votketnPY4s/mark-wright-and-darren-beck-on-the-iot-and-sustainability", 
  "title": "Mark Wright and Darren Beck on the IoT and sustainability"
 }, 
 {
  "content": "O'Reilly Podcast: Qubole founder Ashish Thusoo on the importance of self-service data. What does it take to build a data-driven culture? In this O’Reilly Podcast episode, I pose that question to Ashish Thusoo , founder of Qubole . With his co-founder Joydeep Sen Sarma , Thusoo built a self-service data infrastructure at Facebook beginning in 2007. That infrastructure transformed Facebook’s culture and became part of practically every product decision that the company made. \n\n\n\n “A data-driven culture is a combination of processes, people, and technology that allows companies to bring data into day-to-day conversation,” Thusoo says. “Traditionally, when data was not available, a lot of the decision-making process, both at the tactical and strategic level, would be made through gut intuition. \n\n\n\n “Over a period of time, what has become clear is that, along with intuition, you need to augment that by testing those intuitions and those hypotheses with data. And that is what a data-driven culture enables. ... Data empowers people to actually talk about issues in a substantive way, rather than a subjective way.” \n\n\n\n So, what kind of technology does a company need in order to become data-driven? Infrastructure that’s accessible to anyone in the organization is essential, says Thusoo. “You need to have the infrastructure be self-service to users.” Thusoo’s realization at Facebook back in 2007 was that the company couldn’t scale as long as decision-makers had to ask the data team for answers rather than fetching answers for themselves. “Facebook was an exponentially growing company...so, we had to make sure that we got out from sitting between the users.” \n\n\n\n Even employees who aren’t able to develop their own sophisticated queries might be able to get started by taking pre-written queries and adjusting their parameters. “It’s easy to train people on interfaces that are in tune with their capabilities,” says Thusoo. “The motivation that you give them is...what used to take [your employees] weeks now might take just a few hours.” \n\n\n\n This article and podcast is a collaboration between Qubole and O’Reilly. See our statement of editorial independence. \n Continue reading Building a data-driven culture.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/S4tP51JnXzE/building-a-data-driven-culture", 
  "title": "Building a data-driven culture"
 }, 
 {
  "content": "Computer Security MOOC, Human-Data Interaction, Baidu's Open Source, and Termination of Transfer \n \n Cyber Security Base with F‑Secure -- free computer security MOOC from University of Helsinki and F-Secure. \n \n Human-Data Interaction -- from the Encyclopaedia of Human-Computer Interaction, 2nd ed. \n \n Baidu's Open Source -- interesting to see the different code bases they're working on, and the areas around IoT, \"Internet-scale\" databases, charting, speech, deep learning—all the goodies. \n \n Termination of Transfer (Creative Commons) -- in keeping with this year’s Open Access theme “Open in Action,” Creative Commons and Authors Alliance are pleased to announce a new tool that empowers authors to learn about whether and when they have the right to terminate licensing arrangements they have made with publishers that prevent them from sharing their works openly. \n \n Continue reading Four short links: 26 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/c3_XnTXnxpw/four-short-links-26-october-2016", 
  "title": "Four short links: 26 October 2016"
 }, 
 {
  "content": "Cracking the misconception that hacks are sophisticated and complex by breaking down the most common attack categories. When it comes to the world of hacking, it’s important to understand not just who a hacker is, but also the actual hack itself. Similar to the assumption that hackers are all geniuses, many people assume that their methods are similarly complex and sophisticated. But just as hacker skills range in sophistication, their methods do, as well. \n\n The most unsophisticated examples typically arise because of human error. Take, for instance, the password. After each major login/password breach, analysts review the data and find people use the same passwords. So much so that upward of 5% of people use the same 100 passwords. This means that if you wanted to try to hack into someone’s account, you have a 1 in 20 chance of getting in by just trying the top 100 passwords. As a quick aside, this is very easily stopped if companies occasionally reviewed the most common passwords and didn’t allow users to set them. \n\n Although there are many attack techniques that span sophistication levels, the following is a basic breakdown of some of the most common types of attack categories, including examples. \n\n Social engineering \n\n Social engineering differs from other attacks because it depends on human interaction. Here, the hacker manipulates people into performing an action or divulging confidential information. The hacker relies on people’s natural inclination to help. It’s usually easier to trick someone into giving information rather than hacking for it; for example, fooling someone into revealing a password, rather than attempting to brute-force it by running a computer program that tests hundreds of thousands of password options automatically. \n\n An example of social engineering is phishing . The hacker sends an email that appears to come from a legitimate email address from a trusted organization (a popular choice is a bank), claiming the recipient needs to update a username and password, and provides a convenient link to click. The email might come from a domain like wellsfargo-alerts@passwordrecovery.com that makes it looks official even though it doesn’t come from the Wells Fargo domain. It looks exactly like past emails from Wells Fargo all in an attempt to get the recipient to think it’s real. If the recipient clicks the link, she goes not to the trusted website but to the phisher’s site, which is designed to look legitimate, and provides her private information for the hacker to scoop up and then use to gain access to the actual account. \n\n Network attacks \n\n A network attack is when a hacker performs an intrusion on a network infrastructure or host system. The hacker analyses the network address of the targets, takes advantage of open ports or vulnerabilities, and collects information. These attacks can be passive (in which information is gathered, but not changed) or active (in which information is altered); they can occur from within an organization or from outside. \n\n An example of a network attack is a man-in-the-middle attack. Often seen as MITM, MitM, MIM, MiM attack, or MITMA, a man-in-the-middle attack is when the hacker relays communication between two other parties using the opportunity to capture or modify the data (see Figure 1). The two parties believe they’re communicating with each other, when in reality, the hacker is intercepting and potentially altering the messages. \n\n The hacker completely controls the messages for his own purposes. This could be to gain financial information being relayed to a bank, login information to a website, or any messages encrypted by a public key. \n\n \n Figure 1. A basic man-in-the-middle attack. \n \n\n Web application attacks \n\n A web application attack happens when a hacker targets vulnerabilities to a service that’s connected to the Web (website, mobile application, etc.). Software that used to be installed on a desktop (for example, Microsoft Excel) is rapidly moving to the Internet (Microsoft Office 365 and Google Spreadsheets are run in a web browser instead of a local computer) so that you can access and run it on your computer, phone, or tablet anywhere in the world. Unfortunately, this also means that hackers can easily access it anywhere in the world, as well. As a result, this type of attack has grown in frequency. The application layer, which is easily accessible from the Internet, makes it a particularly soft target. \n\n A SQLi , or SQL injection attack, is an example of a web application hack. A hacker exploits a code flaw (also known as a security bug) in a web application with malicious SQL statements that make the application potentially return any data that’s available in that website’s database (passwords, credit cards, addresses, etc.). \n\n Although this type of attack typically results in stealing a copy of the data to sell, attackers can also use SQLi to tamper with data, such as voiding transactions or changing an account balance. And, in some cases, the hacker can even take over as administrator and controller of the data. \n\n Endpoint attacks \n\n The endpoints of a network make them the least secure; every time someone connects a mobile device to a corporate network, plugs in a USB drive, or downloads an email attachment, a potential hole in the network is created when not done securely. As soon as a hole is created or identified, a hacker can take advantage and install malware onto a network. \n\n An advanced persistent threat (APT) is a type of network attack that relies on vulnerable endpoints. The point of an APT is for the hacker to stay undetected for as long as possible, keeping access to steal a large amount of data. The hacker must continuously rewrite code to stay undetected, making this type of attack time consuming and sophisticated. \n\n Wrap-up \n\n Although this list of attack categories includes the most common methods, it is by no means all-encompassing, nor are attack types static in nature. As long as there’s data worth stealing, there will be people attempting to get at it by whatever means necessary. The bar for how easy or unsophisticated the hack is that’s required to break into a system, however, is dependent on how well defended it is. And, unfortunately, that bar has been dipping to the point where the most basic techniques can be successful. Luckily, many security professionals are working hard to push the bar back up. \n Continue reading The myth of the sophisticated hack.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/e62MjrmmSsQ/the-myth-of-the-sophisticated-hack", 
  "title": "The myth of the sophisticated hack"
 }, 
 {
  "content": "Balancing optimization efforts across processes, technologies, and teams. Much of the software industry seems to be locked in an endless race against time. The tensions between maintaining what we've already built, shipping new work that addresses urgent needs, and keeping an eye on what new opportunities lie just beyond the horizon can leave us feeling overworked and spread thin. \n\n For many tech-centric businesses, this unpleasant way of working is accepted as a way of life. And the default assumption is that a team that cannot easily keep up with the pace of demand is one that needs to grow as soon as possible: that an extra set of hands will help lighten the load, and miraculously cure all problems. This leads many businesses to rush their hiring process, but the expected cure either never comes or is too little, too late. Continue reading Dealing with growing pains without sacrificing sustainability.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/E4LPNzfkv-c/dealing-with-growing-pains-without-sacrificing-sustainability", 
  "title": "Dealing with growing pains without sacrificing sustainability"
 }, 
 {
  "content": "More adventures in deep learning and cheap hardware. The goal of dimensionality reduction is to map high-dimensional data points onto a lower dimensional space. The challenge is to keep similar data points close together on the lower-dimensional mapping. As we’ll see in the next section, our data set contains 13 features. We’ll stick with two dimensions because that’s straightforward to visualize. \n\n Dimensionality reduction is often regarded as being part of the exploring step. It’s useful for when there are too many features for plotting. You could do a scatter plot matrix, but that only shows you two features at a time. It’s also useful as a preprocessing step for other machine-learning algorithms. Most dimensionality reduction algorithms are unsupervised, which means that they don’t employ the labels of the data points in order to construct the lower-dimensional mapping. Continue reading How to build an autonomous, voice-controlled, face-recognizing drone for $200 .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/KqM-iFLLojg/how-to-build-an-autonomous-voice-controlled-face-recognizing-drone-for-200", 
  "title": "How to build an autonomous, voice-controlled, face-recognizing drone for $200"
 }, 
 {
  "content": "EdTech Dictionary, Choose Simple, Dictator's Handbook, and Medical Bioinspiration \n \n A Devil's Dictionary of Educational Technology -- brilliant! Analytics, n. pl. “The use of numbers to confirm existing prejudices, and the design of complex systems to generate these numbers.” \n \n \n Simple Made Easy -- while many choose easiness, they may end up with complexity, and the better way is to choose easiness along the simplicity path. (via Living in the Age of Software Fuckery ) \n \n The Dictator's Handbook (Amazon) -- added to my queue, to read with open source projects in mind. \n \n BioInspiration -- profile of Jeffrey Karp, whose lab at MIT creates useful medical inventions inspired by natural phenomena. I love the focus on practicality. “When we look to solve problems, it’s not so we can publish papers and get pats on the back from the academic community,” said Nick Sherman, a research technician at Karp Lab. “It’s more like, ‘Is this work going to help patients? If not, how do we make it help them?’” \n \n \n Continue reading Four short links: 25 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/s8K-F-A0-00/four-short-links-25-october-2016", 
  "title": "Four short links: 25 October 2016"
 }, 
 {
  "content": "Combining two switches to look like one can be daunting and complex. Not with Arista. Learn how Arista's MLAG makes accomplishing this task very simple. Continue reading An introduction to Arista's MLAG.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/5kcBUETINW4/an-introduction-to-aristas-mlag", 
  "title": "An introduction to Arista's MLAG"
 }, 
 {
  "content": "Learn how to fix a mistake using the “git commit -amend” command and understand when it is appropriate to force push a changed commit.\n Continue reading How do you correct a commit message in Git?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/rjghC5yk0zk/how-do-you-correct-a-commit-message-in-git", 
  "title": "How do you correct a commit message in Git?"
 }, 
 {
  "content": "An interview with Reshmi Krishna, cloud application and platform architect at Pivotal. Continue reading Distributed tracing: What it is and why it will become a default tool.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/aAV_b8O31jc/distributed-tracing-what-it-is-and-why-it-will-become-a-default-tool", 
  "title": "Distributed tracing: What it is and why it will become a default tool"
 }, 
 {
  "content": "Learn eight methods for determining if a string contains a substring in JavaScript. Continue reading How do you check if one string contains a substring in JavaScript?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/BfXeGnPHuRM/how-do-you-check-if-one-string-contains-a-substring-in-javascript", 
  "title": "How do you check if one string contains a substring in JavaScript?"
 }, 
 {
  "content": "Learn how to verify if a checkbox or radio button is checked using the .checked property in JavaScript. Continue reading How do you check if an HTML form checkbox field is checked in JavaScript?.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/_7rbzbOud_s/how-do-you-check-if-an-html-form-checkbox-field-is-checked-in-javascript", 
  "title": "How do you check if an HTML form checkbox field is checked in JavaScript?"
 }, 
 {
  "content": "Soviet Internet, Deep Learning Papers, Chinese Black Mirror, and Coding Conventions \n \n The Internyet -- Soviet scientists tried for decades to network their nation. What stalemated them is now fracturing the global Internet. [...] Glushkov’s story is also a stirring reminder to the investor classes and other agents of technological change that astonishing genius, far-seeing foresight, and political acumen are not enough to change the world. Supporting institutions often make all the difference. There's so much to pull out of this article! \n \n Deep Learning Papers Reading Roadmap -- for those long winter nights that are coming. Lay in supplies of highlighters and cocoa. \n \n China's Plan to Organize its Society Relies on \"Big Data\" to Rate Everyone (WaPo) -- The ambition is to collect every scrap of information available online about China’s companies and citizens in a single place — and then assign each of them a score based on their political, commercial, social, and legal “credit.” Just in time for Black Mirror Season 3, episode 1. \n \n Code Like Shakespeare -- great title, better than any article can be when it's on coding conventions that help people read your code more easily. It's still good advice. \n \n Continue reading Four short links: 24 October 2016.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/0VxDmD5LCD4/four-short-links-24-october-2016", 
  "title": "Four short links: 24 October 2016"
 }, 
 {
  "content": "Rob Harrop looks at how we can use the concepts of CSP, Actors, and Reactive as powerful tools for reasoning about our systems. Continue reading Am I only streaming? Thinking Reactive.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-q49hCJuRqU/am-i-only-streaming-thinking-reactive", 
  "title": "Am I only streaming? Thinking Reactive"
 }
]