[
 {
  "content": "[caption id=\"attachment_80346\" align=\"alignright\" width=\"212\"] Buy Ruby Pocket Reference . [/caption] \n When we talk about blocks in Ruby, we're not usually talking about code blocks — or blocks of statements — as we might with other languages. We're talking about a special syntax in Ruby, and one of its idioms. I'll be discussing blocks in this article, plus a little about procs and lambdas. \n Ruby's blocks are always associated with methods, which are sets of recallable procedures. Blocks can't get along very well by themselves. They are dependent on methods, which normally feed data to them. Without that data, a block can't do anything useful. It needs a parent to look after it. \n Blocks are anonymous and are sometimes referred to as nameless functions . Blocks are like methods within another method that grab data from an enclosing method. If all this is unfamiliar to you, it may not make sense. Keep reading and I'll do my best to clear things up for you. \n \n Block syntax \n Here is an example that uses a block with the each method from Ruby's builtin Array class. The each method is an iterator. An iterator munches data, usually in sequence, and with a little help, can actually do something useful with that data. A block doesn't have to be an iterator, though that is how they are often used. \n First, we'll create an array containing the names of the Western states in the U.S., and then iterate over that array with Array 's each method: \n [crayon-56082082d7b3d262715708/] \n The block parameter e is surrounded by vertical bars. The parameter could have any name you want. (I tend to make mine short.) This particular block uses the parameter locally to keep track of each element in the array west_states , and later uses it to do something with each element of the array, in this case, tidily printing strings to standard output. \n You can write a block with do and end , as shown, or with a pair of braces, as is most commonly done. The braces actually have higher precedence than do / end , and the syntax is more concise, as you can see: \n [crayon-56082082d7b4d637945346/] \n Multiple parameters \n A block may use more than one parameter. Multiple parameters are separated by commas. Here we'll iterate over a hash with Hash 's own each method, where multiple parameters make sense: \n [crayon-56082082d7b55564805757/] \n By the way, hashes are handy containers for key-value pairs, as you might have guessed. Also, each has a synonym in Hash : each_pair . \n Life without blocks \n What happens if you call the Array 's or Hash 's each method without a block? Well, iterators expect blocks. Without one, the each method simply returns an enumerator, nothing more. \n Iterator methods like each don't make much sense without blocks. For example, the upto or downto methods from Integer are fairly useless without blocks. Compare these calls, for example: \n [crayon-56082082d7b5d055566786/] \n With these: \n [crayon-56082082d7b64343657285/] \n Nothing doing with the last two, except returned enumerators. Sort of like watching grass grow. \n Scope \n In Ruby 1.9 or later, if you use as a parameter as a variable name that already exists in the containing scope, the block assigns that parameter each successive value from the object, but the variable's value is unchanged, as you see here (the to_a method converts the range to an array): \n [crayon-56082082d7b6c106768130/] \n So don't worry about variable and parameter names colliding in such instances, unless you are using a pre-1.9 interpreter. \n The yield statement \n As you know by now, a block must follow a method call. But something you might not know is that any method call may be followed by a block, and you can invoke code in such a block with a yield statement. We don't always see yield at work — it is part of the underlying, implicit control structure of iterator methods. But here we'll use it explicitly. \n A yield statement executes a block associated with a method. I'll use some really simple code from my recent book Ruby Pocket Reference, 2nd Edition to illustrate. \n The following method, gimme , contains only a single yield statement and isn't ver exciting: \n [crayon-56082082d7b74693612736/] \n What so far does gimme do? Give gimme a call and find out (I'm doing this in irb , Ruby's homegrown interactive programming environment): \n [crayon-56082082d7b7b026036493/] \n Uh oh. This error showed up because yield 's job is to execute the block that is associated with the method, and that's missing in the code. Avoid this error by using the block_given? method from Kernel . Redefine gimme with an if statement: \n [crayon-56082082d7b83857171859/] \n Try gimme again with a very simple block (not an iterator!) and without: \n [crayon-56082082d7b8a869687801/] \n Now redefine gimme to contain two yield s, and then call it with a block: \n [crayon-56082082d7b91603795181/] \n Another thing you ought know is that after yield executes, control goes back to the statement immediately following it. There's certainly more to say about yield , but I'll leave it at that. \n Do blocks have return values? \n Just a note here, in closing, about return values and blocks. Blocks don't really have return values, not in the same way their parent methods can. If you use a return statement in a block, the containing method will return, not the block. A block yields the value of its last expression. You don't need to use return in a block, nor should you. \n blocks, procs, and lambdas \n A proc is a way to store a procedure in Ruby. Procs are often short, one-liners, though not always. One reason I'm bringing them up here is because a proc is not a proc without a block in Ruby. \n First, a little background. A proc is a first-class object that comes complete with context. As a first-class object, a proc can be created at runtime, stored in data structures, passed as a parameter, and so on. To create a proc, you can call Proc::new , Kernel#lambda , or Kernel#proc . \n The term lambda comes from Alonzo Church's lambda calculus , which famously influenced the development of the Lisp programing language and more recent functional programming languages. Lambda logic can be found in a number of programming languages, including Lisp, Python, Swift, C#, and Ruby, among others. Generally, lambdas are anonymous functions that can be written inline and easily discarded. \n What's the difference between procs and lambdas? Lambdas behave more like methods and procs behave more like blocks, but both are instances of the Proc class. For brevity, I'll only show a lambda here. \n When creating a lambda with the methods mentioned, a block is required. Kernel 's lambda method, for example, expects a block. A call to lambda is equivalent to calling Proc.new and both calls return a proc object. Here is a call to lambda which of necessity includes a block, followed by a call to the new proc: \n [crayon-56082082d7b9a662250941/] \n By the way, since 1.9, you can use the following simplified, lambda literal syntax, with the same result: \n [crayon-56082082d7ba1900377019/] \n There's much to learn about lambdas. I just wanted to show, briefly, how blocks are used with procs. A fuller treatment of procs merits another article. \n Summary \n Let me wrap up with a brief summary of blocks. Blocks are essentially nameless functions that provide a concise way to iterate over objects. An iterator method such as each without a block will return only an enumerator. Blocks have one or more parameters. In addition, a block does not have a return value like a method. It yields the value of its last expression. Finally, stored procedures in Ruby — procs — use blocks as well. \n Thanks for reading. Happy coding. \n \n Note: If you'd like to get more detail, Mike suggests reading section 5.4 on blocks and section 6.5 on procs and lambdas in The Ruby Programming Language by David Flanagan and Yukihiro Matsumoto, plus chapter 8 on blocks in Lucas Carlson's Ruby Cookbook . Both are from O'Reilly. \n Public domain studs image via Pixabay .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/nuDtdZ7BY9U/blocks-in-ruby.html", 
  "title": "Blocks in Ruby"
 }, 
 {
  "content": "Get notified when our free report \"Privacy and Security in the Internet of Things,\" by Gilad Rosner, becomes available. \n \"Without addressing privacy and trust, the Internet of Things will not reach its full potential.\" \n This refrain can be heard at IoT conferences , in opinion pieces in the press and in normative academic literature . If we don't  \"get it right,\" then consumers won't embrace the IoT and all of the wonderful commercial and societal benefits it portends. \n This is false. \n It's a nice idea, imagining that concern for privacy and security will curtail or slow technological growth. But don't believe it: the Internet of Things will develop whether or not privacy and security are addressed. Economic imperative and technology evolution will impel the IoT and its tremendous potential for increased monitoring forward, but citizen concern plays a minor role in operationalizing privacy. Certainly, popular discourse on the subject is important, but developers, designers , policy-makers and manufacturers are the key actors in embedding privacy architectures within new connected devices. \n Unsurprisingly, much current research shows that people are still uncomfortable and feel overexposed regarding their privacy. The prolific Pew Research Center tells us : \n \"...Americans feel privacy is important in their daily lives in a number of essential ways. Yet, they have a pervasive sense that they are under surveillance when in public and very few feel they have a great deal of control over the data that is collected about them and how it is used.\" \n Research across 26 European countries found: \n \"Rather high general perception of risks related to the disclosure of personal information online\" and \"very strong expectations that personal information is used by the website owners / shared with third parties without the users' knowledge and consent.\" \n However, it's very hard to prove that people decline to buy IoT products now or in the future because of these concerns. Such proof would be difficult to obtain: it would require large surveys that not only laid out a common definition of the IoT ( something that experts have yet to agree on ) and then show a strong relationship between consumers declining to purchase something IoT-like and concerns over its privacy and security risks. Or, specific vendors could disclose lackluster sales of their latest devices, ask consumers why they're not buying, and the response must then be poor privacy characteristics. (Such disclosures are not in vendors' interests.) The absence of this proof, however, does not stop professionals and the commentariat from proclaiming this constraint on the growth of connected devices. I call this 'The Orthodoxy of Chilled Innovation.' \n We've seen this before \n The IoT is only the latest market domain in which we hear this orthodoxy. To wit: \n 1995: The global marketplace is doomed! \n \n\"Unless … adequate protection for copyrighted works is ensured, the vast communications network will not reach its full potential as a true, global marketplace.\" (Copyright violation was and is a rampant problem.) \n 2000: Electronic commerce is doomed! \n \n\"The [Federal Trade] Commission believes that its proposed legislation, in conjunction with self-regulation, will ensure important protections for consumer privacy at a critical time in the development of the online marketplace. Without such protections, electronic commerce will not reach its full potential and consumers will not gain the confidence they need in order to participate fully in the electronic marketplace.\" (The proposed legislation never came to pass.) \n 2000: The national information infrastructure is doomed! \n \n\"Unless security and privacy are protected, the [national information infrastructure] won't reach its full potential.\" (Seems to be healthy and evolving.) \n 2004: The networked economy is doomed! \n \"A networked economy will only reach its full potential if sectoral boundaries are dismantled and an even take-up of ICT in society is ensured.\" (There are plenty of uneven socio-economic qualities to the Internet and related technologies.) \n 2010: Online business is doomed! \n \"If we don't get privacy right then the online consumer will revolt, which will negatively impact everyone involved in online businesses.\" (Consumer revolution is a fantasy.) \n Time proved these vague assertions hollow: the Internet, the US national information infrastructure and e-commerce are doing just fine. The privacy and security risks have not been addressed in any radical or comprehensive way, and people are still communicating, buying and surfing. \n The warm fuzzies \n So, what accounts for this orthodoxy? My theory is that it's an attractive, intuitive argument influenced by the collective vulnerability people feel. Starting from the research that says people are worried about the intrusiveness of technology, one can imagine a desire to believe that our worries will translate into a will to slow things down, or a wariness on the part of IoT vendors. The argument that privacy and security must be addressed for the IoT to blossom, then, can be met with head nodding and warm feelings because it assuages fears. \n It is, however, an empty sentiment. The Internet of Things, whatever it is, will happily march along with lousy privacy and security, and we will be the poorer for it. Collective senses of the loss of privacy are a small part of what encourages the improvement of privacy preservation. Certainly, businesses large and small do think about what the populace might find \" creepy ,\" but there is a wide gulf between considering opinions that might affect sales and actually baking privacy into devices. One should not confuse marketing with engineering or business practice. Regarding the IoT, the Orthodoxy of Chilled Innovation ignores recent history and economic logic: businesses seek frictionless transactions, privacy is rarely a differentiator, security and privacy become more opaque topics over time, and businesses behave according to their (absence of) regulatory regimes. The danger of the Orthodoxy is that it may lull people into thinking that something will ensure their sense of privacy loss is addressed before the IoT remakes our world into a digital utopia; a false sense of security. \n Privacy does not protect itself, nor do markets arc toward the social goals of privacy and consumer protection on their own . Privacy is a technocratic pursuit: designers, engineers, product managers, risk and compliance managers, and company leaders are ultimately the ones who can actively improve the privacy posture of their devices. Complementing this are technology-neutral information policies that require privacy impact and security assessments and consumer protection. An unpopular view is that privacy is a paternalistic pursuit by the state. Such a view flies in the face of the economically driven belief that self-regulation is the main force by which we should engender privacy, or that \"[e]ducating and empowering citizens is the better way\" to address privacy failures – two more orthodoxies. Markets in liberal democracies cannot exist without regulation, and regulation itself is not sufficient to effect the protections we seek. Privacy protection occurs through a plurality of necessary but insufficient steps . Wishing is not one of them. \n Public domain image on article and category pages via Pixabay .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/YZAtrFZxxBE/no-the-iot-does-not-need-strong-privacy-and-security-to-flourish.html", 
  "title": "No, the IoT does not need strong privacy and security to flourish"
 }, 
 {
  "content": "Download \"Data Preparation in the Big Data Era,\" a new free report to help you manage the challenges of data cleaning and preparation. \n Data is growing at an exponential rate worldwide, with huge business opportunities and challenges for every industry. In 2016, global Internet traffic will reach 90 exabytes per month , according to a recent Cisco report. The ability to manage and analyze an unprecedented amount of data will be the key to success for every industry. \n To exploit the benefits of a big data strategy, a key question is how to translate all of that data into useful knowledge. To meet this challenge, a company first needs to have a clear picture of their strategic knowledge assets, such as their area of expertise, core competencies, and intellectual property. \n Having a clear picture of the business model and the relationships with distributors, suppliers, and customers is extremely useful in order to design a tactical and strategic decision-making process. The true potential value of big data is only gained when placed in a business context, where data analysis drives better decisions — otherwise, it's just data. \n In a new O'Reilly report Data Preparation in the Big Data Era , we provide a step-by-step guide to manage the challenges of data cleaning and preparation — critical steps before effective data analysis can begin. We explore the common problems of data preparation and the different steps involved, including data cleaning, combination, and transformation. You'll also learn about new products that deal with problem of data variety at scale, including Tamr's solution, which curates data at scale using a combination of machine learning and expert feedback. \n This free report begins by discussing the importance of identifying and exploring your business question. We define various terms used in the data preparation phase, such as raw data , technically correct data , consistent data , tidy data , and aggregated or compressed data . Explore how to select and correct raw data errors, with normalization or string approximation techniques and the value in having a data strategy. \n Download the free report to learn more. \n This post is a collaboration between O'Reilly and Tamr. See our statement of editorial independence . \n Public domain image on article and category pages via Wikimedia Commons .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/OXeyvA-zADU/translating-data-into-knowledge.html", 
  "title": "Translating data into knowledge"
 }, 
 {
  "content": "Police Program Aims to Pinpoint Those Most Likely to Commit Crimes (NYT) -- John S. Hollywood, a senior operations researcher at the RAND Corporation, said that in the limited number of studies undertaken to measure the efficacy of predictive policing, the improvement in forecasting crimes had been only 5% or 10% better than regular policing methods. \n Apple's Assault on Advertising and Google (Calacanis) -- Google wants to be proud of their legacy, and tricking people into clicking ads and selling our profiles to advertisers is an awesome business – but a horrible legacy for Larry and Sergey. Read beside the Bloomberg piece on click fraud and the future isn't too rosy for advertising. If the ad bubble bursts, how much of the Web will it take with it? \n China Is Building The Mother Of All Reputation Systems To Monitor Citizen Behavior -- The document talks about the \"construction of credibility\" — the ability to give and take away credits — across more than 30 areas of life, from energy saving to advertising. \n $9 Computer Hardware (Makezine) -- open hardware project, with open source software. The board’s spec is a 1GHz R8 ARM processor with 512MB of RAM, 4GB of NAND storage, and Wi-Fi and Bluetooth built in.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/v10pyrkWNzo/four-short-links-25-september-2015.html", 
  "title": "Four short links: 25 September 2015"
 }, 
 {
  "content": "[caption id=\"attachment_80326\" align=\"alignright\" width=\"237\"] Download DevOps for Finance . [/caption] \n Download a free copy of DevOps for Finance , an O’Reilly report by Jim Bird for the financial services software insider who's heard about DevOps, but is unsure whether it represents solution or suicide. \n DevOps, until recently, has been a story about unicorns. Innovative, engineering-driven online tech companies like Flickr, Etsy, Twitter, Facebook, and Google. Netflix and its Chaos Monkey. Amazon deploying thousands of changes per day. \n DevOps was originally about WebOps at Internet companies working in the Cloud, and a handful of Lean Startups in Silicon Valley. It started at these companies because they had to move quickly, so they found new, simple, and collaborative ways of working that allowed them to innovate much faster and to scale much more effectively than organizations had done before. \n But as the velocity of change in business continues to increase, enterprises — sometimes referred to as \"horses,\" in contrast to the unicorns referenced above — must also move to deliver content and features to customers just as quickly. These large organizations have started to adopt (and, along the way, adapt) DevOps ideas, practices, and tools. \n This short book assumes that you have heard about DevOps and want to understand how DevOps practices like Continuous Delivery and Infrastructure as Code can be used to solve problems in financial systems at a trading firm, or a big bank or stock exchange. We'll look at the following key ideas in DevOps, and how they fit into the world of financial systems: \n \n Breaking down the \"wall of confusion\" between development and operations, and extending Agile practices and values from development to operations \n Using automated configuration management tools like Chef, Puppet, CFEngine, or Ansible to programmatically provision and configure systems (Infrastructure as Code) \n Building Continuous Integration and Continuous Delivery (CI/CD) pipelines to automatically test and push out changes, and wiring security and compliance into these pipelines \n Using containerization and virtualization technologies like Docker and Vagrant, together with Infrastructure as Code, to create IaaS, PaaS, and SaaS clouds \n Running experiments, creating fast feedback loops, and learning from failure \n \n To follow this book you need to understand a little about these ideas and practices. There is a lot of good stuff about DevOps out there, amid the hype. A good place to start is by watching John Allspaw and Paul Hammond's presentation at Velocity 2009, \"10+ Deploys Per Day: Dev and Ops Cooperation at Flickr\" , which introduced DevOps ideas to the public. IT Revolution's free \"DevOps Guide\" will also help you to get started with DevOps, and point you to other good resources. The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win by Gene Kim, Kevin Behr, and George Spafford (also from IT Revolution) is another great introduction, and surprisingly fun to read. \n If you want to understand the technical practices behind DevOps, you should also take the time to read Continuous Delivery (Addison- Wesley), by Dave Farley and Jez Humble. Finally, DevOps in Practice is a free ebook from O'Reilly that explains how DevOps can be applied in large organizations, walking through DevOps initiatives at Nordstrom and Texas.gov. \n Common challenges \n From small trading firms to big banks and exchanges, financial industry players are looking at the success of Google and Amazon for ideas on how to improve speed of delivery in IT, how to innovate faster, how to reduce operations costs, and how to solve online scaling problems. \n Financial services, cloud services providers, and other Internet tech companies share many common technology and business challenges. \n They all deal with problems of scale. They run farms of thousands or tens of thousands of servers, and thousands of applications. No bank — even the biggest too-big-to-fail bank — can compete with the number of users that an online company like Facebook or Twitter supports. On the other hand, the volume and value of transactions that a major stock exchange or clearinghouse handles in a trading day dwarfs that of online sites like Amazon or Etsy. While Netflix deals with massive amounts of streaming video traffic, financial trading firms must be able to keep up with low-latency online market data that can peak at several millions of messages per second, where nanosecond accuracy is necessary. \n These Big Data worlds are coming closer together, as more financial firms like Morgan Stanley, Credit Suisse, and Bank of America adopt data analytics platforms like Hadoop. Google (in partnership with SunGard) is one of the shortlisted providers bidding on the Securities and Exchange Commission's new Consolidated Audit Trail (CAT), a secure platform that will hold the complete record of every order, quote, and trade in the US equities and equities options markets: more than 50 billion records per day from 2,000 trading firms and exchanges, all of which needs to be kept online for several years. This will add up to several petabytes of data. \n The financial services industry, like the online tech world, is viciously competitive, and there is a premium on innovation and time to market. Businesses (and IT) are under constantly increasing pressure to deliver faster, and with greater efficiency — but not at the expense of reliability of service or security. Financial services can look to DevOps for ways to introduce new products and services faster, but at the same time they need to work within constraints to meet strict uptime and performance service-level agreements (SLAs) and compliance and governance requirements. \n DevOps tools in the finance industry \n DevOps is about changing culture and improving collaboration between development and operations. But it is also about automating as many of the common jobs in delivering software and maintaining operating systems as possible: testing, compliance and security checks, software packaging and configuration management, and deployment. This strong basis in automation and tooling explains why so many vendors are so excited about DevOps. \n A common DevOps toolchain includes: \n \n Version control and artifact repositories \n Continuous Integration/Continuous Delivery servers like Jenkins, Bamboo, TeamCity, and Go \n Automated testing tools (including static analysis checkers and \nautomated test frameworks) \n Automated release/deployment tools \n Infrastructure as Code: software-defined configuration management tools like Ansible, Chef, CFEngine, and Puppet \n Virtualization and containerization technologies such as Docker \nand Vagrant \n \n Build management tools like Maven and Continuous Integration servers like Jenkins are already well established across the industry through Agile development programs. Using static analysis tools to test for security vulnerabilities and common coding bugs and implementing automated system testing are common practices in developing financial systems. But as we'll see, popular test frameworks like JUnit and Selenium aren't a lot of help in solving some of the hard test automation problems for financial systems: integration testing, security testing, and performance testing. \n Log management and analysis tools such as Splunk are being used effectively at financial services organizations like BNP Paribas, Credit Suisse, ING, and the Financial Industry Regulatory Authority (FINRA) for operational and security event monitoring, fraud analysis and surveillance, transaction monitoring, and compliance reporting. \n Automated configuration management and provisioning systems and automated release management tools are becoming more widely adopted. CFEngine, the earliest of these tools, is used by 5 of the 10 largest banks on Wall Street, including JP Morgan Chase. Puppet is being used extensively at the International Securities Exchange, NYSE, E*Trade, and the Bank of America. Bloomberg, the Standard Bank of South Africa (the largest bank in Africa), and others are using Chef. Electric Cloud's automated build and deployment solutions are being used by global investment banks and other financial services firms like E*Trade. \n While most front office trading systems still run on bare metal in order to meet low latency requirements, Docker and other containerization and virtualization technologies are being used to create private clouds for testing, data analytics, and back office functions in large financial institutions like ING, Société Générale, and Goldman Sachs. \n Financial players are truly becoming part of the broader DevOps community by also giving back and participating in open source projects. For example, LMAX, who we will look at in more detail later, has open sourced its automated tooling and even some of its core infrastructure technology (such as the low-latency Disruptor inter-thread messaging library). And at this year's OSCON, Capital One released Hygieia , an open source Continuous Delivery dashboard. \n Financial operations is not WebOps \n Financial services firms are hiring DevOps engineers to automate releases and to build Continuous Delivery pipelines, and Site Reliability Engineers (patterned after Google) to work in their operations teams. But the jobs in these firms are different in many ways, because a global bank or a stock exchange doesn't operate the same way as Google or Facebook or one of the large online shopping sites. Here are some of the differences: \n \n Banks or investment advisers can't run continuous, online behavioral experiments on their users, like Facebook has done. Something like this could violate securities laws. \n DevOps practices like \"Monitoring as Testing\" and giving developers root access to production in \"NoOps\" environments so that they can run the systems themselves work for online social media startups, but won't fly in highly regulated environments with strict requirements for testing and assurance, formal release approval, and segregation of duties. \n Web and mobile have become important channels in financial services — for example, in online banking and retail trading — and web services are used for some B2B system-to-system transactions. But most of what happens in financial systems is system-to-system through industry-standard electronic messaging protocols like FIX, FAST, and SWIFT, and low-latency proprietary APIs with names like ITCH and OUCH. This means that tools and ideas designed for solving web and mobile development and operations problems can't always be relied on. \n Continuous Deployment, where developers push changes out to production immediately and automatically, works well in stateless web applications, but it creates all kinds of challenges and problems for interconnected B2B systems that exchange thousands of messages per second at low latencies, and where regulators expect change schedules to be published up to two quarters in advance. This is why this book focuses on Continuous Delivery: building up automated pipelines so that every change is tested and ready to be deployed, but leaving actual deployment of changes to production to be coordinated and controlled by operations and compliance teams, not developers. \n While almost all Internet businesses run 24/7, most financial businesses, especially financial markets, run on a short trading day cycle. This means that a massive amount of activity is compressed into a small amount of time. It also means that there is a built-in window for after-hours maintenance and upgrading. \n While online companies like Etsy must meet PCI DSS regulations for credit card data and SOX-404 auditing requirements, this only affects the \"cash register\" part of the business. A financial services organization is effectively one big cash register, where almost everything needs to be audited and almost every activity is under regulatory oversight. \n \n Financial industry players were some of the earliest and biggest adopters of information technology. This long history of investing in technology also leaves them heavily weighed down by legacy systems built up over decades; systems that were not designed for rapid, iterative change. The legacy problem is made even worse by the duplication and overlap of systems inherited through mergers and acquisitions: a global investment bank can have dozens of systems performing similar functions and dozens of copies of master file data that need to be kept in sync. These systems have become more and more interconnected across the industry, which makes changes much more difficult and riskier, as problems can cascade from one system — and one organization — to another. \n In addition to the forces of inertia, there are significant challenges and costs to adopting DevOps in the financial industry. But the benefits are too great to ignore, as are the risks of not delivering value to customers quickly enough and losing them to competitors — especially to disruptive online startups powered by DevOps. We'll start by looking at the challenges in more detail, to understand better how financial organizations need to change in order for them to succeed with DevOps, and how DevOps needs to be changed to meet their requirements. \n Then we'll look at how DevOps practices can be — and have been — successfully adopted to develop and operate financial systems, borrowing ideas from DevOps leaders like Etsy, Amazon, Netflix, and others. \n \n Disclaimer: The views expressed in this book are those of the author, and do not reflect those of his employer or publisher.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-kRhtlepY4M/devops-for-finance.html", 
  "title": "DevOps for Finance"
 }, 
 {
  "content": "The Web welcomes , but it's awfully big. While HTML, CSS, and JavaScript may all be appropriate entry points for newcomers who want to create, finding a solid starting point can be complicated. Social media has minimized the level of HTML and Web knowledge people need to start contributing, but when it's time to make the jump, the Web offers perhaps too many options. \n Part of the challenge is that HTML, CSS, and JavaScript may be the marquee technologies, but they're not actually what hosts a website or app. Setting up a site requires an additional set of technologies, from domain names to hosting to web server choices. Setting up a site - long before you get to packaging an app! - requires mastering an additional technical toolset and vocabulary that will help you navigate where you need to put your projects. Our free report, Getting Started with the Web , provides the core foundations beginners need. \n Those aren't the only barriers, though. Earlier this year, Rachel Andrew worried that: \n We're cutting off those easy entries into becoming a really good web developer because you can use so much stuff now... you read any tutorial and you need Grunt, you need all these other things, you can't just write CSS. I got into the Web just because I wanted to build websites, and I viewed source and I started building website. You still can do that. I want to make sure there's still those easy routes in so that people can start playing with things, and don't feel the need to have a huge stack of stuff before they can write a line of HTML. \n Tools are just part of the challenge, though - the ever-expanding universe of devices on which people view the Web makes it all more daunting. Andrew addressed that too : \n that battle about \"what do we do with these phones?\" ...we sidestepped [making multiple sites] with responsible design but there are still issues of performance, things we're trying to work out as a community. I think it's the best thing we have to tackle the issue. \n The good news on the device front is that HTML itself is quite nicely responsive, though creating attractive formatting that adjusts itself naturally across device contexts is trickier. In some ways, though, it's easier to learn responsive design early , when you don't have to unlearn past techniques, than to bolt it on later. Getting started right will make it much easier when it's time to optimize. \n HTML5, which revitalized HTML a few years ago, started as an effort to improve HTML form handling, and then grew rapidly beyond that. Support for audio and video removed the need for (slow and risky) browser plugins just as Flash, the main way people had delivered multimedia content, was having a very bad few years at the hands of Apple. Managing large audio and video files remains a challenge of its own, but including them in a document has become much more like including an image in a document - manageable. \n Would you like to learn these things, building sites (and perhaps eventually apps) that will look good and last through years of maintenance? O'Reilly's HTML5 Fundamentals Learning Path can get you started.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/7m5mgRhvzyE/learning-the-web.html", 
  "title": "Learning the Web"
 }, 
 {
  "content": "Subscribe to the O'Reilly Radar Podcast to track the technologies and people that will shape our world in the years to come. \n In this week's Radar Podcast, O'Reilly's Ben Lorica talks to Paco Nathan , director of O'Reilly Learning, and Jesse Anderson , technical trainer and creative engineer at Confluent. \n Their discussion focuses on the training landscape in the big data ecosystem, their teaching techniques and particular content they choose, and a look at some expected future trends. \n Here are a few snippets from their chat: \n Training vs PowerPoint slides \n Anderson: \"Often, when you have a startup and somebody says, 'Well, we need some training,' what will usually happen is one of the software developers will say, 'OK, I've done some training in the past and I'll put together some PowerPoints.' The differences between a training thing and doing some PowerPoints, like at a meetup, is that a training actually has to have hands-on exercises. It has to have artifacts that you use right there in class. You actually need to think through, these are concepts, these are things that the person will need to be successful in that project. It really takes a lot of time and it takes some serious expertise and some experience in how to do that.\" \n Nathan: \"Early on, you would get some committer to go out and do a meetup, maybe talk about an extension to an API or whatever they were working on directly. If there was a client firm that came up and needed training, then they'd peel off somebody. As it evolved, that really didn't work. That kind of model doesn't scale. The other thing too is, you really do need people who understand instructional design, who really understand how to manage a classroom. Especially when it gets to any size, it's not just a afterthought for an engineer to handle.\" \n Framing the training \n Anderson: \"[Companies are] starting to look for, 'OK, we've done Hadoop or we have Hadoop already in place, we want to start doing things in more real time.' So, they're starting to look at the real-time frameworks like Spark streaming and, in some cases Flink , but what we're generally going to see is that a lot of these new technologies that are coming out that enable more real-time processing of big data, this is what they're going after. One of those big technologies is Kafka. Part of the reason why I partnered with Confluent to do the Kafka training, I'd go into companies and they'd say 'Okay, here's our general architecture diagram and it all starts out with Kafka and goes into either Hadoop or Spark or both.'\" \n Nathan: \"People want to focus on the context. Really, what's the business context and what kind of architectural patterns work for the right kind of use cases? As Jesse was mentioning, I think that maybe the people who make a given framework aren't necessarily going to be the ones who are as invested in showing all the different integrations possible. That's the borderlands. I do find that people come to training to find out more about what integrations are possible, what ones make sense. Like we were mentioning about Spark and Kafka, maybe put Cassandra in that mix. That's a pattern that shows up in banking, it shows up in genomics. It shows up all over. It's a good illustration of the integrations that people are really craving.\" \n Educating the enterprise \n Anderson: \"One of the big gaps is that we've done great things for engineers and for analysts and for some of the operations people, but we really haven't dealt with the business people. I saw this when I was teaching at Cloudera; I'd have managers or even sometimes director-level people that publicly traded companies sitting through my entire four-day class. I thought, they either have a great love of knowledge or they're really wasting their time — they could have gotten about what they needed in a day or so. ... A business person needs to have a general idea how these technologies work, but more importantly, they need to have an idea of how you make money at it. \n I created a course I call the Business of Big Data to address that. We go through the technology at a high level, but it's the applications of the technology that we focus on. It's how do you use technology, big data technology, in order to improve things. ... How do I use data in order to make a decision? What I call a data augmented decision. I'm going to use data in order to augment whatever decision I'm making but how do you work back from there? How do you create a successful big data project? One of the things I've seen is that companies will have difficulty carrying out that big data project because it's quite a bit different than other ones. It involves more technology. It involves more expertise and I found that some of them would have a difficult time in actually carrying that out.\" \n Nathan: \"I've definitely been involved in in-house training for companies where it's hard to tell where the training stops and where the consulting starts. That's fair game. I think there's a lot of room for that because people really want to understand the context before they could ever jump into a contract. One of the things there is if a team internally comes to a training session, then they can start to explore some of those issues that they might want to follow up with a contractor for. \n Another thing to embellish on something that Jesse was mentioning earlier, another really good reason why people come to these kinds of training outside the customized environment, though, is to hear what other people ask. There's a lot of value in that. I'm not sure if I should use the term, but the 'unknown unknowns.' If you can hear the pain points that other companies and other teams are experiencing and hear how the experts answer them, you might actually save your company a lot of time.\" \n \n Subscribe to the O'Reilly Radar Podcast \n Stitcher , TuneIn , iTunes , SoundCloud , RSS \n \n \n Related: \n \n Business of Big Data Workshop — Live online training with Jesse Anderson, November 10, 2015, from 9 a.m. to 3 p.m. PT. \n Learning Path: Architect and Build Big Data Applications — This Learning Path will take you through the entire process of designing and building data applications that can visualize, navigate, and interpret reams of data. \n \n Public domain image on article and category pages via Wikimedia Commons.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/19AheT9gyEM/training-in-the-big-data-ecosystem.html", 
  "title": "Training in the big data ecosystem"
 }, 
 {
  "content": "Subscribe to the O'Reilly Data Show Podcast to explore the opportunities and techniques driving big data and data science. \n \n I first found myself having to learn Scala when I started using Spark (version 0.5). Prior to Spark, I'd peruse books on Scala but just never found an excuse to delve into it. In the early days of Spark, Scala was a necessity — I quickly came to appreciate it and have continued to use it enthusiastically.\n \n For this Data Show Podcast, I spoke with O'Reilly author and Typesafe's resident big data architect Dean Wampler about Scala and other programming languages, the big data ecosystem, and his recent interest in real-time applications. Dean has years of experience helping companies with large software projects, and over the last several years, he's focused primarily on helping enterprises design and build big data applications.\n \n Here are a few snippets from our conversation:\n \n Apache Mesos & the big data ecosystem \n It's a very nice capability [of Spark] that you can actually run it on a laptop when you're developing or working with smaller data sets. … But, of course, the real interesting part is to run on a cluster. You need some cluster infrastructure and, fortunately, it works very nicely with YARN. It works very nicely on the Hadoop ecosystem. … The nice thing about Mesos over YARN is that it's a much more flexible, capable resource manager. It basically treats your cluster as one giant machine of resources and gives you that illusion, ignoring things like network latencies and stuff. You're just working with a giant machine and it allocates resources to your jobs, multiple users, all that stuff, but because of its greater flexibility, it cannot only run things like Spark jobs, it can run services like HDFS or Cassandra or Kafka or any of these tools. … What I saw was there was a situation here where we had maybe a successor to YARN. It's obviously not as mature an ecosystem as the Hadoop ecosystem but not everybody needs that maturity. Some people would rather have the flexibility of Mesos or of solving more focused problems.\n \n \n Tachyon \n I think it's still early days, but I think the potential is there. In a way, it's analogous to Spark in that it starts with some really good fundamental ideas and then builds on them. In Spark's case, it would be the so-called resilient distributed data sets. With Tachyon, it's basically an in-memory distributed file system — or a way to think of it, it's like a distributed cache with file system semantics. What's attractive about that is that you can basically have multiple applications accessing the same data sets and memory, accessing them through a file system, kind of API or a more proprietary API, but you get in-memory speeds with some configuration to do some durability. Behind the scenes, obviously you don't want that data to get lost if the machine goes down. There's facilities for having the data be backed to a file system, so I think it solves a number of interesting problems in big data applications like sharing data between running jobs, like giving you much more flexibility and performance characteristics. I think it's pretty exciting.\n \n Backpressure and reactive streams \n Backpressure would be signaling from the consumer back to the producer, 'Hey, I can't take as much data as you're feeding me, or I can actually take more data.' It's this protocol for controlling the rate of flow. And the reason this is important is because a classic way of implementing a connection between a producer and consumer is to put a buffer, like a queue in between them but then you have this dilemma. You could make it unbounded so that you never fill it up but the problem is memory is finite. Inevitably when you think about what's going to happen in a stream system that runs for years, some weird situation … where the producer will just keep feeding data too fast to a consumer and it'll eventually run out of memory and crash. You don't like that but the flipside is, all right, make these things bounded buffers but you still haven't completely solved your problems because then what do you do when that fills up? You end up arbitrarily dropping data or doing some other thing. \n ... \n The idea with backpressure is, just have a negotiation happen out of band, like separate socket connection or something, where when the consumer can keep up, it's just a push model. I just keep pushing data, but if the consumer gets backed up, then the consumer can signal, 'All right, send me five more or send me 10 more,' or whatever, that kind of thing, until [it] gets caught up. \n [ Reactive streams ] is a standard for that backpressure mechanism [so that] if you get a directed graph of these things, then you can make strategic decisions at the beginning. If I've got data coming into this system  and I'm getting backpressure, at least I can make a strategic decision about what to do. \n \n Subscribe to the O'Reilly Data Show Podcast \n Stitcher , TuneIn , iTunes , SoundCloud , RSS \n \n \n Related resources: \n \n Why the data center needs an operating system by Benjamin Hindman, creator of Apache Mesos \n Introduction to Tachyon and a deep dive into Baidu's production use case : a recent O'Reilly webcast co-presented by Haoyuan Li, the co-creator of Tachyon. \n Showcasing the real-time processing revival: Tools and learning resources for building intelligent, real-time products (sessions at Strata+Hadoop World NYC) \n Apache Spark: Powering applications on-premise and in the cloud , a Data Show episode featuring Spark's release manager, Patrick Wendell. \n \n You can listen to our entire interview in the SoundCloud player above, or subscribe through Stitcher , SoundCloud , TuneIn , or iTunes . \n Image on article and category pages by L Rempe-Gillen on Wikimedia Commons .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Qeq1cufOoEI/building-enterprise-data-applications-with-open-source-components.html", 
  "title": "Building enterprise data applications with open source components"
 }, 
 {
  "content": "Download our new free report \"Mapping Big Data: A Data Driven Market Report\" for insights into the shape and structure of the big data market. \n Who are the major players in the big data market? What are the sectors that make up the market and how do they relate? Which among the thousands of partnerships are most important? \n These are just a handful of questions we explore in-depth in the new O'Reilly report now available for free download: Mapping Big Data: A Data Driven Market Report . For this new report, San Francisco-based startup Relato mapped the intersection of companies throughout the data ecosystem — curating a network with tens of thousands of nodes and edges representing companies and partnerships in the big data space. \n Relato created the network by extracting data from company home pages on the Web and analyzed it using social network analysis; market experts interpreted the results to yield the insights presented in the report. The result is a preview of the future of market reports. \n The report begins with an introduction to the data set and a history of the market. We present a market segmentation by cluster analysis, which shows the sub-markets within the data ecosystem, with similar vendors in like clusters. We go on to analyze how these markets connect with one another and explore some of the surprising insights. \n The report ranks and compare the major players in the market — those who have enabled the development of the market — Hadoop platform vendors Cloudera, Hortonworks, and MapR. You'll see how we compare and contrast vendors using traditional metrics and various types of centrality. The report characterizes the vendors' partnership networks and will give you a better understanding of the vendors themselves. \n No longer do market analyses need to rely only on surveys and produce completely subjective quadrant charts. This report reveals that market reports can be based on data collected directly from the footprint of the market players themselves, in their course of doing business. \n Download Mapping Big Data: A Data Driven Market Report . \n Screenshot image on article and category pages via Relato's interactive map .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/dQVdlDsFxm8/major-players-and-important-partnerships-in-the-big-data-market.html", 
  "title": "Major players and important partnerships in the big data market"
 }, 
 {
  "content": "[caption id=\"attachment_80286\" align=\"alignright\" width=\"341\"] Abbott and Costello performing \"Who's on First?\" [/caption] \n Abbott and Costello's signature wordplay sketch \"Who's on First?\" is one of the most renowned comedic routines of all time. Trying to describe the routine here will do it little justice, you'll just have to watch it yourself . As funny as it may be, the sketch reveals a crucial fact: names are important. Good names should be self-explanatory, precise and reveal intent. Bad names leave people confused and aggravated and should be avoided at all cost. When we write code, we must always think about variable names, function names, file names, etc. But naming things is hard. Phil Karlton probably said it best: \"There are only two hard things in Computer Science: cache invalidation and naming things.\" \n Since Node.js was first released in 2009, many developers have started discussing and implementing ways to share JavaScript code between the web application server and the browser. This sharing of JavaScript code allows for assembling web pages on either the client or server, with the benefits of faster initial page load times and improved search engine optimization. The name of this approach was coined by Charlie Robbins and later popularized by Spike Brehm as \"isomorphic\" JavaScript. Such applications are isomorphic in the sense that they take on equal (ἴσος: isos) form or shape (μορφή: morphe) regardless of which environment they are running on, be it the browser or the server. \n \n \"Isomorphic\" JavaScript also emphasizes that sharing code between the server and the client requires differentiating environment-specific code. \"Isomorphism\" captures the notion of two mathematical objects that have corresponding or similar forms when we simply ignore their individual distinctions. When applying this mathematical concept to graph theory it becomes easy to visualize. Take for example the two graphs in the following figure. \n [caption id=\"attachment_80287\" align=\"alignnone\" width=\"566\"] Example of isomorphic graphs. [/caption] \n These graphs are isomorphic, even though they look much different. For starters, they both have the same number of nodes with each node containing the same number of edges. They are isomorphic because each node in the first graph maps to a corresponding node in the second graph while maintaining certain properties. For example, the node A can be mapped to node 1 while maintaining its adjacency in the second graph. In fact, all nodes in the graph have an exact one-to-one correspondence while maintaining adjacency. \n This is what's nice about the \"isomorphic\" analogy. In order for JavaScript code to run both on the client and server environments, these environments have to be isomorphic, that is, there should exist a mapping of the client environment to the server environment functionality, and vice versa. Just as the two isomorphic graphs have a mapping, so do isomorphic JavaScript environments. For JavaScript code that does not depend on environment specific features, for example, avoids using the \"window\" or \"request\" objects, it can easily run on both sides of the wire. But for JavaScript code that accesses environment-specific properties, e.g., \"req.path\" or \"window.location.pathname,\" a mapping needs to be provided to abstract or \"fill in\" (sometimes referred to as a \"shim\") a given environment-specific property. Isomorphic JavaScript applications may take advantage of build scripts and tools like Browserify and Webpack , which extract, morph, and package code up into a browser-friendly version. \n \"Isomorphic\" JavaScript, however, has left some confused and aggravated with its semantic ambiguity. Some have raised concerns over the word's inaccuracy and the barrier it creates for new developers to grasp the concept. Many have suggested alternative names. Michael Jackson, a React.js trainer and co-author of the react-router project , has suggested the term \"universal\" JavaScript . This term borrows from Apple's description of app bundles that ran on different CPU architectures (i.e. PowerPC and Intel) and of apps that run on both the iPad and the iPhone. Because of Apple's usage and the familiarity of the term in everyday use, Michael has had better luck explaining \"universal\" JavaScript to the developers he's trained. The term \"universal\" also highlights JavaScript code that can run \"not only on servers and browsers, but on native devices and embedded architectures as well\". This gives the name a distinct flavor and helps in expanding the discussion beyond the traditional web application client-server model. \n Yet others have suggested names like \"shared\" or \"portable\" JavaScript to stress the commonality of the code running on the server and the client. \"Cross-environment\" and \"Full-stack\" JavaScript have also come up in this naming discussion. Each of these names has its own merit, and it's exciting to see the desire of the JavaScript community for good names. For our upcoming O'Reilly book we've decided to continue using isomorphic JavaScript. It's a term that emphasizes the right things for us, has become part of the vernacular and is better known than what's been suggested so far. But what do you think? Please leave a comment below with your suggestions and help us name our book. \n \n Public domain \"Who's on First?\" image via Wikimedia Commons .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VDB7M0l1FOA/renaming-isomorphic-javascript.html", 
  "title": "Renaming isomorphic JavaScript?"
 }, 
 {
  "content": "Contrary to what many believe, insights are difficult to identify and effectively apply. As the difficulty of insight generation becomes apparent, we are starting to see companies that offer insight generation as a service. \n Data storage, management and analytics are maturing into commoditized services, and the companies that provide these services are well-positioned to provide insight on the basis not just of data, but data access and other metadata patterns. \n Companies like DataHero and Host Analytics [full disclosure: Host Analytics is one of my portfolio companies] are paving the way in the insight-as-a-service space. Host Analytics' initial product offering was a cloud-based Enterprise Performance Management (EPM) Suite, but far more important is what they are now enabling for the enterprise: they have moved from being an EPM company to being an insight generation company.  In this post, I will discuss a few of the trends that have enabled insight as a service (IaaS) and discuss the general case of using a software-as-a-service (SaaS) EPM solution to corral data and deliver insight as a service as the next level of product. \n Insight generation is the identification of novel, interesting, plausible and understandable relations among elements of a data set that a) lead to the formation of an action plan and b) result in an improvement as measured by a set of KPIs. The evaluation of the set of identified relations to establish an insight, and the creation of an action plan associated with a particular insight or insights, needs to be done within a particular context and necessitates the use of domain knowledge. \n Insight as a service refers to action-oriented, analytic-driven, cloud-based solutions that generate insights and associated action plans. Insight as a service is a distinct layer of the cloud stack (I've discussed IaaS in earlier posts here and here ). In the case of Host Analytics, its EPM solution integrates a customer's financial planning data with actuals from their Enterprise Resource Planning (ERP) applications (e.g., SAP or NetSuite, and relevant syndicated and open source data), creating an insight-as-a-service offering that complements their existing solution. EPM, in other words, is not just a matter of streamlining data provisions within the enterprise; it's an opportunity to provide a true insight generation solution.\n \n EPM has evolved as a category much like the rest of the data industry: from in-house solutions for enterprises to off-the-shelf but hard-to-maintain software to SaaS and cloud-based storage and access. Throughout this evolution, improving the financial planning, forecasting, close, and reporting processes continue to be a priority for corporations. EPM started, as many applications do, in Excel but gave way to automated solutions starting about 20 years ago with the rise of vendors like Hyperion Solutions. Hyperion's Essbase was the first to use OLAP technology to perform both traditional financial analysis as well as line-of-business analysis. Like many other strategic enterprise applications, EPM started moving to the cloud a few years ago. As such, a corporation's financial data is now available to easily combine with other data sources, open source and proprietary, and deliver insight-generating solutions.\n \n The rise of big data — and the access and management of such data by SaaS applications, in particular — are enabling the business user to access internal and external data, including public data. As a result, it has become possible to access the data that companies really care about, everything from the internal financial numbers and sales pipelines to external benchmarking data as well as data about best practices. Analyzing this data to derive insights is critical for corporations for two reasons. First, great companies require agility, and want to use all the data that's available to them. Second, company leadership and corporate boards are now requiring more detailed analysis.\n \n Legacy EPM applications historically have been centralized in the finance department. This led to several different operational \"data hubs\" existing within each corporation. Because such EPM solutions didn't effectively reach all departments, critical corporate information was \"siloed,\" with critical information like CRM data housed separately from the corporate financial plan. This has left the departments to analyze and report and deliver their data to corporate using manually integrated Excel spreadsheets that are incredibly inefficient to manage and usually require significant time to understand the data's source and how they were calculated rather than what to do to drive better performance.\n \n In most corporations, this data remains disconnected. Understanding the ramifications of this barrier to achieving true enterprise performance management, IaaS applications are now stretching EPM to incorporate operational functions like marketing, sales, and services into the planning process. IaaS applications are beginning to integrate data sets from those departments to produce a more comprehensive corporate financial plan, improving the planning process and helping companies better realize the benefits of insight as a service. In this way the CFO, VP of sales, CMO, and VP of services can clearly see the actions that will improve performance in their departments and, by extension, elevate the performance of the entire corporation.\n \n Image on article and category pages via Wellcome on Wikimedia Commons .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/9HpRAR0R5Ss/improving-corporate-planning-through-insight-generation.html", 
  "title": "Improving corporate planning through insight generation"
 }, 
 {
  "content": "During a special edition of The O'Reilly Podcast , host and O'Reilly chief data scientist Ben Lorica interviewed Dr. Clare Bernard, a former particle physicist at CERN, who worked on the ATLAS experiment at the Large Hadron Collider. Bernard is now a field engineer at Tamr, where she's involved in a new project that aims to integrate and catalog a variety of data across an enterprise, while preserving metadata. \n Key takeaways from their chat: \n \n A lot of companies have big top-down master data management projects, and they put in place a lot of data-governance tools, which typically don't scale very well. \n It's really important to track where the data came from, what the fields mean, and what transformations have been applied to that data over time, so that you can then use it for your analytics and you really understand what it means. \n Tracking metadata allows you to reproduce your data pipelines, and understand the lineage, and provenance of your data. \n \n Ben Lorica: Let's start with a little bit about your background. You are a scientist by training, right? \n Clare Bernard: Yes. I was a particle physicist. I worked at CERN for a couple years and worked on the ATLAS experiment at the Large Hadron Collider. Then I got my Ph.D. and graduated in May. I've been working at Tamr since then, as a field engineer. \n BL: When you were a physicist, were you more on the computational side of physics? \n CB: Yes, I was in a data-scientist-type role. I did help a little bit with collecting the data, but mostly, I spent my time analyzing the data and coming up with measuring standard model physics processes, and searching for new types of particles that we might be able to discover. \n BL: What kind of tools did you use in the academic world, for that type of work? \n CB: Particle physicists typically don't buy the type of software that is used in the private world — we write a lot of it ourselves. Particle physicists use a framework called Root — it's a statistical analysis package. \n BL: Is this like a distributed framework, like the type used in big data? \n CB: Yes, you can create jobs that you send off to really large computing clusters. CERN has a huge system of distributed computing, so there's tier ones and tier twos, and you send data all around the world to do your analysis. \n BL: What made you decide not to go into an academic profession? \n CB: I really enjoyed my work in grad school and living at CERN, but I was very interested in figuring out what else I could do with my skills. Programming and data analysis translates well to the private world of big data. \n While I was at CERN, we discovered the Higgs-Boson, which was really exciting to be a part of. Some of the other experiments proposed for the future are more related to precision measurements, rather than looking for new particles, which was what I was particularly interested in. \n BL: Today, you've worked with several Fortune 500 companies as a Tamr field engineer. What are your thoughts on the state of enterprise data in most large organizations? \n CB: Most large organizations are capturing huge amounts of data, and there is a tremendous amount of value in that data, but these enterprises are having a really difficult time getting at that value. They have very messy data that's in many different systems. Then, on the other side of the enterprise, they've got analysts and data scientists who want to derive value from the data, but who have trouble figuring out what data is there, and figuring out how to get that data and bring it together. Understanding how to deal with data quality issues, and getting data into a usable format, have been big hurdles for many companies. \n A major challenge is to embrace some of the messiness of enterprise data, and some of the variety of that data, so that we can really unlock all of the potential. \n BL: What tools and systems are companies using for organizing and visualizing their data assets? \n CB: There are a lot of patterns you see across enterprises. A lot of companies have big top-down master data management projects, and they put in place a lot of data-governance tools, which typically don't scale very well. \n Another challenge is too many systems, and then even within one system you can have duplicate records about something like a customer. Figuring out where all of the data is located, when you have many different systems, is a very challenging problem. \n BL: How effective have data-governance tools been, in terms of managing different data sources, of variable quality? \n CB: One big initiative that a lot of companies have started on is centralizing all of their data into a central repository (the data lake). \n Getting the data into one place makes it a little bit easier to figure out what's there, but if it's not curated, it becomes even worse because now you've taken the data away from the places they were collected, and you've lost a lot of the information about how the data was collected.   \n BL: Actually, this is a topic that I've started to pay attention more to: metadata. \n CB: Yes, exactly. A lot of companies are realizing that this is an issue, and they're trying to track all of the metadata about the data they're putting into HDFS, into their data lake. \n BL: How are they doing that? \n CB: A lot of companies are trying to build metadata catalogs. There are a few metadata catalogs, and they're specifically made for HDFS. Very few of these that are in the market have been successful, so far. \n One big issue for this, is that you don't really want just the data that's in your data lake and HDFS. You really want to be able to connect to all of the systems. You want it to be an easy adoption. \n BL: Let's set the stage for why metadata, and good metadata management, is important. \n CB: Absolutely. Let's say we have data about customers in a data lake, with fields called \"Given Name,\" \"Last name,\" and then there's a field called, \"Name.\" We don't really know where to start there. Which of all these name fields is actually the name of the customer? \n In reality, what's happened is, someone has created an ETL process that has taken this data from its original source system, has brought it into the data lake, and they've done some transformations on this name field, but now we've lost all of the information about what transformations were done and how that data has changed, and where the data was initially collected. Now, as the data scientist, it's a little difficult to trust that data, and to know how to use it and how to get value out of it. \n It's really important to track where the data came from, what the fields mean, and what transformations have been applied to that data over time, so that you can then use it for your analytics and you really understand what it means. \n BL: This allows you to reproduce your data pipelines, understand lineage, and provenance of your data. \n CB: Yes. You really want to be able to leverage the other work that other people in the enterprise have been doing. You may not have just one data scientist, you may have hundreds of data scientists. If one data scientist is working on a project to clean up one data set and apply some useful transformations to it, then you really want all of your other data scientists to be able to take advantage of that work. \n BL: What you're saying is, not only am I exposing it to the other data scientists in my company, I'm also exposing it in a way that they can understand what I've done to the data. \n CB: Exactly, because not all of your data scientists are doing the exact same project. They don't necessarily want the exact same transformations. They just want to be able to see what has been done by everyone else and leverage that work if it applies to their particular project. \n BL: What are the tools for making that happen? Generally, what would the tools look like that explain to my colleagues what I have done to the data? \n CB: I think there are a lot of tools that address parts of this issue. This catalog, as I'm describing it, doesn't really exist. One of the reasons we've gotten interested in this problem is because this is something I see repeatedly when I go to customer sites — it takes them a really long time to find the data to get started. \n Catalogs that go across systems and that capture all of the metadata, including comments and collaborative features — I don't think that something like that exists in enterprise today. \n BL: There's an open source academic project right now that's just starting, by Joe Hellerstein, one of the co-founders of Trifacta that attempts to do this; that's something you can share across different frameworks. \n Obviously, you folks at Tamr are thinking about this issue, and other folks are thinking about it, so it must be because companies and enterprises are struggling with this or asking for this. \n CB: Yes, definitely. One of the things that happens a lot in meetings is that we'll show one of the initial screens of the Tamr Connect product, which just has a list of the sources that you've connected to the system, and sometimes we get a very emotional reaction, where people say something like: \"Oh, this is what I want. I just want a list of the sources that I have.\" \n My initial reaction was, \"Wait, really?\" This is not supposed to be the screen that you get excited about. This is just a screen that says, \"This is the data that we're working with.\" \n That's a problem that people are increasingly focused on, especially as more and more people have initiatives where they're trying to bring data into the data lake, and they're realizing that as they do that, they're losing a lot of context. \n BL: What is Tamr's free Catalog software, and how does it solve problems for discovering, organizing, and visualizing data? \n CB: Customers often ask us: how do we integrate our data sets and make them into the type of data set that a data scientist would want to use? The Catalog product is a very lightweight, easy-to-use web application that is focused on the problem of discovering, organizing, and visualizing the data in the enterprise. \n The organization can share Catalog as a centralized repository of metadata, and then associate human knowledge with that data. It's focused on connecting to as many systems as possible, and then capturing collaborative insights. \n BL: Is machine learning part of the catalog software? \n CB: The Catalog software is more focused on profiling, and right now doesn't have a whole lot of machine learning. Right now, Catalog is about connecting to a lot of systems, giving you a list of sources, and then helping you get value out of that list of sources, so you can figure out which attributes can be associated with a particular entity. \n The vision going forward is that it will be more connected with Tamr's Connect product — there will be good integration between the two, and there will certainly be machine learning components in both. \n BL: Have you gone to some of your customers and talked to them about Catalog? If so, what kind of response are they seeing? \n CB: Yes, we actually already have 550 registrants and over 300 users from a wide variety of organizations. We've been working really closely with some of these Alpha customers to make sure that what we're building is solving their problems. We've had a lot of really good feedback about being able to increase data awareness and the level of organization of data in the enterprise. \n BL: What do companies have in existence right now to solve the problems we've been discussing? \n CB: A lot of the tools that are being used to solve these problems are bigger, top-down, rule-based, data-governance approaches, that are maintained by a small number of administrators and start to break down at scale. The core Tamr difference is that Catalog can connect to any system. It's free, so we're really interested in driving adoption across the enterprise. \n Catalog is really only part of the solution. We start with Catalog, and then once you see where all the data is across your enterprise, and you see how this data relates to the projects that you want to do, then you can use the Tamr Connect product to connect those data sources and produce those data sets that are useful for your analytics. \n BL: Data sources could run the gamut of different types of systems, right? \n CB: Yes, you can download Tamr Catalog on your laptop to try it out, and connect to any XML, CSV, and Excel files that are on your laptop. You can connect to various databases, as well. The vision of the product is to connect to everything. We want to integrate with everything across the enterprise and really embrace that variety. \n You can download Tamr's Catalog product for free here , and you can listen to the complete podcast episode in the player below. \n \n This post is a collaboration between O'Reilly and Tamr. See our statement of editorial independence . \n Cropped image on article and category pages via Nevit Dilmen on Wikimedia Commons .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/np2VMlLTQKk/integrate-catalog-and-preserve-metadata.html", 
  "title": "Integrate, catalog, and preserve metadata"
 }, 
 {
  "content": "Buy \"The Hardware Startup: Building Your Product, Business, and Brand,\" by Renee DiResta, Brady Forrest, and Ryan Vinyard. Note: this post is an excerpt from the book. \n Hardware founders should strive for hypothesis-driven development and validate their ideas with customers as early as possible. This can be done before or alongside the prototyping process, but it should absolutely be done before moving toward tooling. \n Changing your design or switching technologies becomes much more expensive once you've moved beyond a prototype, so it's important to get it right the first time. \n The foundation of effective validated learning is conversation . You want to talk to — and, more importantly, listen to — several distinct groups of people who will be critical to your success as a company. \n Your fellow hardwarians \n The first group of helpers on the road to building your product are fellow hardware founders. Not everyone within the hardware community is a potential customer who will help you unearth the roots of a particular pain point. But within this community are the people who have done it before. They have extensive experience and can provide invaluable guidance for specific problems. They'll help you reduce waste in the production process. Certain steps in the development process, such as finding a contract manufacturer, are often driven by word-of-mouth referrals. Networking with other founders building products in your space will give you a better chance of getting these things right the first time. \n Many first-time hardware founders are coming from software, or are hobbyist makers who haven't developed extensive personal networks within the hardware startup community. Fortunately, it's easier than ever to find fellow hardwarians. Although online communities can have strong ties, they aren't a substitute for a local network. Even if you live in a smaller town, meeting fellow hardware hackers nearby can help you discover facilities, suppliers, and other resources. These connections are particularly valuable early on because you'll likely be prototyping locally. \n The Hackerspace Wiki is a great resource for finding like-minded people, and it has the added benefit of potentially helping you discover a shared workspace or machine shop. See if there's a TechShop or FabLab in your area. In addition to teaching classes, these spots often host community events such as show-and-tells and happy hours. The Hardware Startup Wiki and The Maker Map can also help you discover what's happening locally; these two sites are maintained in part by the authors of this book, but they rely heavily on community contribution. \n Your co-founder and team \n If you already have a co-founder, or an early founding team, that's awesome! If not, you're going to need to find one. It can take several months to convince someone to join your team, particularly if you haven't received funding. A co-founder relationship is like a marriage: you're going to be building a company with this person for years, so it's important to find an equally committed partner and establish good channels of communication right from the start. \n If you're looking for a co-founder, consider signing up for a co-founder matching community such as FounderDating , CoFoundersLab , or Collab-Finder . \n Building a well-rounded team right from the start is crucial. In the early days, this involves finding people you work well with, who have complementary skill sets. Know what you yourself bring to the table as well as what you need. At a minimum, you need one founder with hardware experience and another with business experience (or a second technical founder who's willing to learn). \n If you're technical, you probably have a good sense of what it takes to build a product. However, you might not understand sales, branding, or marketing very well, and at some point, someone needs to convince people to buy your device. Having a well-rounded business development person as an early team member can help ensure that you don't neglect this critical part of company-building. \n If you're a nontechnical founder, you absolutely must recruit a technical partner. It will be virtually impossible to raise funding or efficiently get to market without one (in terms of both time and cost). A technical co-founder shouldn't be a hired gun who you bring on to build your idea. You need to attract a teammate who shares your vision and wants to partner with you. Sales and finance experience isn't particularly useful in the early days of the company, so it's important to spec out specifically what you will be doing while your co-founder builds. \n If you're planning to build a connected device, wearable, or any product that interfaces with your customer via software, you'll want to bring on a software engineer as early as possible, preferably one with some UI/UX design experience. Software is the arena where a hardware company can continue to innovate even after the physical product has been shipped, which is incredibly important. Some founders choose to outsource software development in the beginning, but you'll need to line up someone to own that critical product role fairly early on. \n It's beneficial to build a pipeline of potential talent right from the start. Talent acquisition is yet another reason to get plugged into your local hardware community. Increasingly common in most cities, hackathons are a great low-pressure way to work on a project and meet people. If you went to a university with a strong alumni network, join your school's listserve or LinkedIn group. \n Your LinkedIn connections are a powerful resource to leverage. The broader your network, the more likely you'll find potential teammates with the skill set you're looking for. If you see someone you'd like to meet, ask for an introduction from a shared contact, even if you're not immediately hiring. \n As you begin to meet people, be open. Keeping your idea a secret won't get you very far. If you're passionate about your vision, talk about it with the potential teammates you're meeting. You need to know if their vision aligns with yours. Ideally, they'll bring their own ideas about the product to the table as well. When building a founding team, you're looking for collaborators, not people who will rely on you to make all of the decisions and drive 100% of the early development. \n Once you've connected with potential teammates, work on some small projects to see if you mesh well together. Founder incompatibility is one of the most common reasons young companies fail. You must discuss the questions of equity splits and division of labor as early as possible. Putting it off isn't going to make the conversation any easier. \n One common concern is commitment : are both/all co-founders quitting their jobs to do this full time? Quitting a paying job before funding is secured is risky, but a founder who does so is able to devote 100% of his or her time and attention to launching the new company. If one founder is working on the startup full-time while the other is still at another paying job, it is common for the founder(s) who took the risk to get more of the equity. \n Your mentors \n Another group of people to reach out to early in the idea stage are potential mentors. A good mentor will offer meaningful advice and help you work through challenges. He or she will typically have experience in some facet of your market or business. A mentor might have a particular skill set (for example, a marketing expert), deep domain knowledge (for instance, a doctor who specializes in the ailment your device remedies), years of experience, or valuable industry connections. For a first-time hardware entrepreneur, building a close relationship with someone who has been through the process of manufacturing a product (and knows the pitfalls to avoid) is invaluable. As is the case with fellow hardwarians, a mentor can also help you reduce waste and prevent overburden in your production process. \n Reaching out to potential mentors can be daunting, particularly if you're doing so via a cold email. People with a lot of experience are often in senior leadership roles and might seem inaccessible. For most of them, time is a valuable commodity in short supply. Therefore, your approach really matters. Telling someone you want him to be your mentor right off the bat is the wrong approach; it's like proposing marriage on the first date. Strong mentor relationships develop over time. When you reach out for the first time, follow these pointers: \n \n Keep the email or message concise and to the point. Long emails are often ignored, or the recipient postpones reading them until it's convenient. \n Tell the person why you're reaching out to them specifically, to avoid the impression that your email is a mass mailing. \n Be clear with your ask. Ask for help with a discrete question or clearly defined problem. Ideally, this should be something that justifies a personal response, rather than an issue they have already addressed in a blog post or other public source of information. \n Minimize friction by offering to accommodate their schedule and preferred mode of communication. \n \n Some startup founders choose to formalize an advisory relationship and recognize a mentor's contribution by awarding a small equity stake in the company. This should involve a formal contract, in which the founder and mentor agree on the specific level of engagement that the mentor is expected to maintain. Monthly meetings or calls are considered standard. Strategic help, such as participation in recruiting or introduction to customers, or expert value-adds, such as work on a particular project, typically increase the amount of equity offered. \n Don't make the mistake of choosing advisors because they are famous and you want their picture on a slide to impress investors. If you're going to give away part of your company to this person, make sure they're doing enough work to merit the award. Be sure that your advisory contract includes criteria for termination in case the relationship goes sour and the advisor doesn't deliver, or if you pivot into a space in which the advisor's expertise is no longer relevant. \n Your True Believers and early community \n Your True Believers are your earliest evangelists. They're the people who care enough to actively help you on the road to success, right from the start.  The core of this group is usually your personal friends and family. \n Gathering your True Believers requires taking a close look at your personal network. Start small: tell your closest friends and family what you're doing and ask if they'd be interested in regular updates. Then, widen the circle. If you already have a following on social media channels, leverage it: tweet, post, drive friends and followers to a landing page with a forum or email list signup. Identify the influencers in your network who will likely be interested in your project, and reach out to them individually via a dedicated note. Tell them what you're doing and why you're excited about it. \n There's no magic ideal number of True Believers. However, since one of the functions of this group is to help you get the word out, increasing the number of True Believers can increase your reach, so it's worth spending time cultivating these relationships. The more engaged this group is, the easier it will be to successfully run a crowdfunding or preorder campaign down the road. You should start to involve the people with whom you have a strong relationship as early as possible, even if you don't have much to share at first. \n True Believers are a subset of a bigger group: your early-adopter community. Communities typically form around interests, practices (for instance, hobbies or professions), or location (geographic proximity). Sometimes, they are working toward a shared goal and can be considered communities of action. Or, alternatively, they might be bound by shared circumstances, such as sexual orientation. It's important to identify why your earliest adopters should want to spend their time participating in your community. What will they get out of the experience? They are probably interested in your product, but they won't want to spend a big chunk of their free time endlessly discussing something that doesn't exist yet. While True Believers care about you , the slightly broader early adopter community generally cares deeply about your space . As a result, they're an excellent source of unvarnished feedback for early idea validation. \n Getting a new community off the ground can feel like an insurmountable obstacle; it's really hard to build a group from nothing. To grow your community beyond the True Believers who love you, you'll have to put some effort into extending and strengthening your network. If you don't have a strong circle of contacts, or have avoided developing a presence on popular social channels, consider putting in a small amount of time each day to foster one. You don't have to become a social media guru, but having a rich network can only help you. \n Establish relationships by participating in online groups. Post interesting content on Twitter, and engage with people who are commenting or sharing articles about your space. Google a wide variety of keywords and phrases, see what blogs or sites exist, and reach out to the authors. Look for in-person meetups. If you don't find any, set one up. You might find kindred spirits in academia as well; for example, many universities have robotics and other hardware-focused clubs. \n Once you've identified a handful of potential early adopters, it's time to facilitate dialogue and begin turning the group into a true community. Introduce the members to each other and seed a conversation. You're going to have to do the work of keeping conversations going until there are enough engaged users that it happens organically. Tailor your early content accordingly. To the extent that you're comfortable, talk about the development process you're going through, and solicit feedback (again: idea validation!). Passionate community members will enjoy feeling like they're part of the process. You should try to maintain a level of excitement that will keep them evangelizing for you through what is potentially a long path to market. \n Work hard to avoid making your community discussions all about you or your product because that will get boring quickly. Discuss the broader industry or space. Share news articles about the underlying issues you all care about. Call attention to the milestones or successes of the other members of the group. Above all, make your True Believers and early adopters feel valued and heard. Consider offering special perks to show appreciation, particularly as you begin to take pre-orders or ramp up to a crowdfunding campaign. \n To keep the group growing, encourage early members to refer trusted contacts from their own networks. Reach out to your top community members individually to ask if there's anyone you should invite to participate in the group. Having an invitation-only model in the beginning can help you control the size and tone as well as establish community norms. As the group grows and you no longer have to personally drive engagement, you can make it public and open and begin to promote it. \n It's easy to host your group using one of the many free tools available online. Setting up a Google or Facebook group is quick and effortless. Creating a Ning or blog-based forum requires more work, but it's more customizable. It's also likely to rank high in search results, which can help people to find you. \n If you have a project that will appeal to a very technical or Maker-oriented audience, consider creating a forum on Instructables . Whatever platform you choose, make sure it's a format that your community is receptive to. Once the community is public, be sure to facilitate easy sharing of content to other networks, so that others can discover you. \n Related: \n \n Brady Forrest and Renee DiResta on Advising Hardware Startups — podcast \n Bazillion Dollar Club on SyFy \n Watch a fireside chat featuring authors Renee DiResta, Brady Forrest, and Ryan Vinyard from Solid 2015: \n \n https://youtu.be/NMmZiJB4wU8 \n Public domain image on article and category pages via Pixabay .", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/88W7I3T4I2w/the-validated-learning-process-for-building-a-hardware-startup.html", 
  "title": "The validated learning process for building a hardware startup"
 }, 
 {
  "content": "The Hit Charade (MIT TR) -- Spotify’s deep-learning system still has to be trained using millions of example songs, and it would be perplexed by a bold new style of music. What’s more, such algorithms cannot arrange songs in a creative way. Nor can they distinguish between a truly original piece and yet another me-too imitation of a popular sound. Johnson acknowledges this limitation, and he says human expertise will remain a key part of Spotify’s algorithms for the foreseeable future. \n The Future of War is the Distant Past (John Birmingham) -- the Naval Academy is hedging against the future by creating cybersecurity midshipmen, and by requiring every midshipman to learn how to do celestial navigation. \n What Happens Next Will Amaze You (Maciej Ceglowski) -- the next in Maciej's amazing series of keynotes, where he's building a convincing case for fixing the Web. \n Go Will Dominate the Next Decade (Ian Eyberg) -- COBOL OF THE 2020s. There, I saved you the trouble.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/O3LVg0Otr18/four-short-links-24-september-2015.html", 
  "title": "Four short links: 24 September 2015"
 }, 
 {
  "content": "Download a free copy of our new report \"User Experience Design for the Internet of Things,\" by Claire Rowland, to learn about a framework for understanding the UX of consumer IoT products. Note: this post is an excerpt from the report. \n When we think of design for connected products, we tend to focus on the most visible and tangible elements. These are the industrial design of connected devices, and the user interfaces (UIs) found in mobile and Web apps and on the devices themselves. \n They are important concerns, which have a major impact on the end user's experience of the product. But they're only part of the picture. You could create a beautiful UI, and a stunning piece of hardware, and users could still have a poor experience of the product as a whole. \n Designing for IoT is inherently more complex than Web service design. Some of this is to do with the current state of the technology. Some of this reflects our as-yet immature understanding of compelling consumer IoT value propositions. Some of this stems from the fact that there are more aspects of design to consider. Tackling them independently creates an incoherent user experience (UX). \n Designing a great connected product requires a holistic approach to user experience. It spans many layers of design, not all them immediately visible. More than ever, it requires cross-discipline collaboration between design, technology, and business. Great UX may start with understanding users. But the designer's ability to meet those users' needs depends on the technology enablers, business models, and wider service ecosystem. \n As designers and their collaborators, we need a shared understanding of the challenges. We also need a common vocabulary for discussing them so that when we use the word \"design,\" we're talking about the same things. \n This report introduces a framework for understanding the experience design of consumer IoT products. It sets out the different facets of design that combine to shape a connected product, and shows you how they fit together. It explains the extra complexities that surround designing for connected products. And it discusses how technology and the wider commercial context work to shape the UX of IoT products. \n It's beyond the scope of this report to delve into the design process for IoT. This is more complex than pure software design: hardware adds new considerations and is less easily modified. Value propositions and design requirements must be clearly defined before product and design decisions are baked into the hardware, when they are hard to change. But here, we will show why the nature of the challenges requires collaboration between design and engineering for both hardware and software, and the business. \n Why UX for IoT is different \n Connected products pose design challenges that will be new to designers accustomed to pure software services. Many of these stem from: \n \n The specialized nature of IoT devices \n Their ability to bridge the digital and physical worlds \n The fact that many IoT products are distributed systems of multiple devices, and \n The quirks of networking. \n \n How tricky those challenges prove will depend on: \n \n The maturity of the technology you're working with \n The context of use, and the expectations your users have of the system, and \n The complexity of your service (for example, how many devices the user has to interact with) \n \n But for most connected products, you'll need to consider the following factors: \n Specialized devices, with different capabilities \n Many of the 'things' in the internet of things are specialized embedded computing devices. Unlike general-purpose computers (smartphones and PCs), their hardware and software is optimized to fulfill specific functions. \n Their physical forms must be designed and engineered. Their UI capabilities may extend from screens and buttons into physical controls, audio, haptics, gestures (see Fig. 1-1), tangible interactions and more. But user interactions must be designed without the benefit of the style guides and standards that Web and mobile designers can rely upon. Some may have no user input or output capabilities at all. The only way to find out what they are doing or what state they are in may be via a remote UI. \n [caption id=\"attachment_80239\" align=\"aligncenter\" width=\"570\"] Fig. 1-1: Jared Ficklin demonstrating Room-E: a system combining voice and gestural interactions. A gesture (pointing at a lamp) is combined with voice input (“computer, turn off this light”) to turn off the correct lamp. Image: Taylor Hamilton, Frog, and Jared Ficklin. [/caption] \n Real world context \n Connected products exist in the physical world. Sensors enable us to capture data we did not have before for digital transmission, allowing us to take more informed actions in the real world. Actuators provide the capability for digital commands to produce real world effects (see Fig. 1-2). They can be remotely controlled, or automated. But unlike digital commands, real-world actions often cannot be undone. \n [caption id=\"attachment_80240\" align=\"aligncenter\" width=\"570\"] Fig. 1-2: Sensors convert readings from the physical environment into digital information; actuators convert digital instructions into mechanical actions. [/caption] \n The physical context of use creates further challenges. Devices may need to be ruggedized for outdoor use. An in-car system needs to be designed to minimize distraction while driving. A remotely controlled oven needs to minimize the risk of fire. Devices must adhere to regulatory requirements such as radio interference or waste recycling standards. And the social context of use may be particularly complex, especially in the home. Techno-centric solutions that are insensitive to the needs of the occupants will fail. \n For example, an assisted living product needs to balance the need of vulnerable people for safety and support, while preserving their privacy and autonomy. Automated rules and modes in some smart home systems perform actions when certain conditions are met, like turning devices on or off when people arrive home, wake up or leave. And permissions in some smart home systems allow an \"admin\" user to grant or deny access to certain devices to others in the house, such as controlling TV/games console time for children or locking cupboards containing dangerous things. But both of these often fail to take into account that real life, especially in families, is often messy and unpredictable. It's not always possible to predict which devices will be needed or not needed at different times. And in most families, permissions are often flexible and negotiated. Few people enjoy feeling like sysadmins for their own homes. \n Designing for systems of devices and services \n Many connected products are systems of diverse devices and services. Functionality may be distributed across multiple devices with different capabilities. \n Designers need to consider how best to distribute functionality across devices. They need to design UIs and interactions across the system as a whole – not treating devices as standalone UIs – to ensure that the overall UX is coherent. This is interusability . And much of the information processing for an IoT product will often happen in the internet service. So the whole system experience is often equally or more important than any single device UX. \n Furthermore, they need some understanding of how the system works. Even quite simple connected products are conceptually more complex than non-connected ones. Code can run in more places. Parts of the system will inevitably go offline from time to time. When this happens, basic knowledge of which component does what will help users understand the consequences, and figure out what action may be required. \n Many connected products support automation, for example home automation rules that turn devices on and off in response to pre-set triggers.  Users may have to keep track of a Web of interrelationships between devices to predict, understand and fix undesired clashes and strange behaviors. \n Over the last 30 years, the prevailing trend in UI design has been direct manipulation (PDF) . Users control visual representations of objects and immediately see the outcome of their actions, which can be reversed. But many IoT interactions are displaced in location (remote control) or time (automation). This breaks the link between user actions and visible, reversible consequences we have come to expect from modern software (PDF) . \n Complex products, like a connected home system, can have many users, multiple UIs, many devices, many rules and applications. Understanding and managing how they all interrelate can be extremely difficult. \n Aside from the configuration overhead this imposes on users, this is a cognitive challenge. Most of us are good at thinking about concrete things. But we are less good at understanding systems and interrelationships, and predicting the future consequences of our actions. \n Designing for networks \n Another major factor is the impact of the network on UX. Designers from Web and mobile software backgrounds have the luxury of assuming that devices will be nearly always connected. And most users understand that sometimes the internet, as experienced through PCs or mobiles, can be slow, or unreliable. Emails can be slow to download and Skype calls can fail. When latency and reliability problems do occur, they may be frustrating but are not unexpected, and can be worked around. \n Our experience of the physical world is that things respond to us immediately and reliably. Light switches do not 'lose' our instructions or take 30 seconds to produce an effect. Delays and glitches are inherent properties of physical networks and transmission protocols. But they may feel strange experienced through 'real world' things. It's impossible to engineer these issues entirely out of any internet-connected system (see Fig 1-4). \n [caption id=\"attachment_80242\" align=\"aligncenter\" width=\"570\"] Fig 1-4. The design and product company BERG produced a beautiful concept prototype for a connected washing machine. The video shows instant responses between the mobile app and washing machine, running over the Internet. In a real world situation, this could not be guaranteed. Images: Timo Arnall of BERG. [/caption] \n In addition, the nature of connected devices is that they often connect only intermittently, in order to conserve power. Computers promise to provide us with precise, accurate and timely data about the world around us. But distributed IoT systems may not always be in sync, and different devices may therefore report different information about the state of the system. \nIn a distributed system, designers must often consider delays, discontinuities and uncertainty as part of normal user interactions and handle them as elegantly as possible.", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Zn4d1nrn21Y/understanding-the-experience-design-of-consumer-iot-products.html", 
  "title": "Understanding the experience design of consumer IoT products"
 }
]